{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf9ee1d-2027-43f5-96e4-d91233d0a20c",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# 0. Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3da4ba2-a841-4fe9-986c-9605ec3d9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "import methods     # for custom methods.\n",
    "\n",
    "# Random Seed.\n",
    "from transformers import set_seed\n",
    "import tensorflow as tf\n",
    "\n",
    "set_seed(42)              # For HF.\n",
    "tf.random.set_seed(42)    # For tf, np, and python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0f687-9942-451e-aeeb-3664c1365756",
   "metadata": {},
   "source": [
    "> #### Note) All defines or method is in './methods.ipynb' file.   \n",
    "> These are supposed to be imported (e.g. methods.plot_learning_curve).   \n",
    "> But for convenience, it is also defined in the last of 'main.ipynb', and directly used.   \n",
    "> Apology for possible inconvenience, I was barely familiar with competitions :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e6b33-7f08-4f06-9d99-572d1a4a6910",
   "metadata": {},
   "source": [
    "#### To print tables on the left, inside of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68286540-618c-4f3e-9b57-5cb50a610cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        float: left;\n",
       "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1a0db-7c14-4dfd-9986-b6c9ddf7efe8",
   "metadata": {},
   "source": [
    "# 2. Preprocessing.\n",
    "- Choose nonlinguistic features : **Category** and **Host**.\n",
    "- Preprocess : One-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26731822-8af4-4b42-a928-f1c3031480d4",
   "metadata": {},
   "source": [
    "## 2.1. Load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3672c5-8fc9-4909-91c2-d945f73da998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any null values? False\n"
     ]
    }
   ],
   "source": [
    "# Load.\n",
    "from sklearn.model_selection import train_test_split\n",
    "data                = pd.read_csv('./dataset/train.csv')\n",
    "train_set, test_set = train_test_split(data, test_size=0.2)\n",
    "\n",
    "label_col_idx = train_set.columns.get_loc('question_asker_intent_understanding')  # Label Columns start from it.\n",
    "x_train       = train_set.iloc[:, :label_col_idx]\n",
    "y_train       = train_set.iloc[:, label_col_idx:]\n",
    "x_test        = test_set.iloc[:, :label_col_idx]\n",
    "y_test        = test_set.iloc[:, label_col_idx:]\n",
    "\n",
    "# Null Check.\n",
    "is_null = data.isnull().values.any()\n",
    "print(f'Any null values? {is_null}')\n",
    "\n",
    "# Copy train_set for EDA. (Only if train_set < 2 GB)\n",
    "train_set_size = train_set.memory_usage(deep=True).sum() / (1024 ** 3)  # In GB.\n",
    "\n",
    "if train_set_size < 2:               \n",
    "    train_cp   = train_set.copy()\n",
    "    x_train_cp = x_train.copy() \n",
    "    y_train_cp = y_train.copy()\n",
    "else:\n",
    "    print(\"Train set copy failed! It's more than 2 GB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1576b2-c0bc-42ee-9b76-4308efc9a1e7",
   "metadata": {},
   "source": [
    "> #### Note) Load Data with ðŸ¤—.  \n",
    "> In practice, use `datasets.load_dataset()`, then `df = ds.to_pandas()`.  \n",
    "> `df` loads directly to RAM, but 'ds' uses memory-mapping with disk.  \n",
    "> And conversion only temporary changes the interface, not actual conversation.  \n",
    "> Simply put, **it's a way more efficient.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711ba50-2e5b-4842-bf6d-46e8eb9986d0",
   "metadata": {},
   "source": [
    "## 2.2. Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4d4dec-9415-4001-8fbe-c8231a77330f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qa_id                                                              1369\n",
       "question_title                           Get raw output from web server\n",
       "question_body         For research I am looking to get the actual ra...\n",
       "question_user_name                                            beingalex\n",
       "question_user_page                 https://serverfault.com/users/111705\n",
       "answer                Add the --save-headers option to the wget comm...\n",
       "answer_user_name                                        Michael Hampton\n",
       "answer_user_page                   https://serverfault.com/users/126632\n",
       "url                             http://serverfault.com/questions/430316\n",
       "category                                                     TECHNOLOGY\n",
       "host                                                    serverfault.com\n",
       "Name: 861, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show features.\n",
    "x_train_cp.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f175f0-e24b-4470-b317-5542bf790c94",
   "metadata": {},
   "source": [
    "### 2.2.1. Delete Redundant features.\n",
    "- `qa_id` : Just ID of the sample.\n",
    "- `question_user_name` : Not useful since it is not equal with rater.\n",
    "- `question_user_page` : Same reason.\n",
    "- `answer_user_name`   : For real competition, we could try to extract some info from the user page, but not for practice.\n",
    "- `answer_user_page`   : Same reason.\n",
    "- `url` : Same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b2d908b-19b1-45ea-b383-3aa44b3f0392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_title', 'question_body', 'answer', 'category', 'host'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete redundant features.\n",
    "redundant_features = ['qa_id', 'question_user_name', 'question_user_page',\n",
    "                      'answer_user_name', 'answer_user_page', 'url']\n",
    "x_train_cp = x_train_cp.drop(columns=redundant_features)  \n",
    "x_train_cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a241ec-f93f-49ae-a152-17cb3a384897",
   "metadata": {},
   "source": [
    "### 2.2.2. Encode Categorical Columns.\n",
    "- `question_title`, `question_body`, `answer` : Should remain as string.\n",
    "- `category` : Only 5, let's encode.\n",
    "- `host` : There are clearly dominant sources. Let's try top $n$ and else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5532aa-c9d2-40ce-a0e4-380599518f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 5\n",
      "category\n",
      "TECHNOLOGY       1947\n",
      "STACKOVERFLOW    1015\n",
      "CULTURE           770\n",
      "SCIENCE           573\n",
      "LIFE_ARTS         558\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length: 63\n",
      "host\n",
      "stackoverflow.com                1015\n",
      "english.stackexchange.com         188\n",
      "electronics.stackexchange.com     184\n",
      "superuser.com                     178\n",
      "serverfault.com                   175\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['question_title', 'question_body', 'answer', 'category_CULTURE',\n",
       "       'category_LIFE_ARTS', 'category_SCIENCE', 'category_STACKOVERFLOW',\n",
       "       'category_TECHNOLOGY', 'host_Others', 'host_askubuntu.com',\n",
       "       'host_electronics.stackexchange.com', 'host_english.stackexchange.com',\n",
       "       'host_math.stackexchange.com', 'host_physics.stackexchange.com',\n",
       "       'host_rpg.stackexchange.com', 'host_serverfault.com',\n",
       "       'host_stackoverflow.com', 'host_superuser.com',\n",
       "       'host_tex.stackexchange.com'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of each class.\n",
    "cols_cat = ['category', 'host']\n",
    "for col in cols_cat:\n",
    "    print(f'Length: {len(x_train_cp[col].value_counts())}')\n",
    "    print(x_train_cp[col].value_counts()[:5], end='\\n\\n')\n",
    "\n",
    "# Encode `category` and `host`.\n",
    "# Find Top N hosts.\n",
    "n_hosts   = 10\n",
    "top_hosts = x_train_cp['host'].value_counts().nlargest(n_hosts).index\n",
    "\n",
    "# Convert others into 'Others'.\n",
    "x_train_cp['host'] = x_train_cp['host'].apply(lambda x: x if x in top_hosts else 'Others')\n",
    "\n",
    "# One-hot Encoding.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cols_to_enc = ['category', 'host']\n",
    "one_enc     = OneHotEncoder(handle_unknown='ignore')          # Zero vector for unknown category.\n",
    "x_enc       = one_enc.fit_transform(x_train_cp[cols_to_enc])\n",
    "\n",
    "# Convert back to DataFrame.\n",
    "enc_columns = one_enc.get_feature_names_out(cols_to_enc)\n",
    "x_enc_df    = pd.DataFrame(x_enc.toarray(), columns=enc_columns, index=x_train_cp.index)\n",
    "\n",
    "x_train_cp   = x_train_cp.drop(columns=cols_to_enc)           # Drop original 'category' and 'host' columns.\n",
    "x_train_cp   = pd.concat([x_train_cp, x_enc_df], axis=1)      # Concatenate the encoded columns back to x_train_2.\n",
    "\n",
    "# Result.\n",
    "x_train_cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589e4d5-5574-46f5-8ee7-44499354fae0",
   "metadata": {},
   "source": [
    "### 2.2.3. Merge txt columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b744b6d9-4778-4260-beb7-8bd13cde9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sentences into one column.\n",
    "cols_txt   = ['question_title', 'question_body', 'answer']\n",
    "x_train_cp['txt_merged'] = x_train_cp[cols_txt].apply(lambda row: ' '.join(row), axis=1)\n",
    "x_train_cp = x_train_cp.drop(columns=cols_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8a654-c6b8-4b09-bf74-0e888f50d96e",
   "metadata": {},
   "source": [
    "## 2.3. Preprocessing Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55f7f6f4-3d08-4b1f-8a99-d2e886f7990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess(x_train, n_hosts=10):\n",
    "    # 2.2.1. Drop redundant features.\n",
    "    redundant_features   = ['qa_id', 'question_user_name', 'question_user_page', \n",
    "                            'answer_user_page', 'answer_user_name', 'url']\n",
    "    x_train = x_train.drop(columns=redundant_features)\n",
    "\n",
    "    # 2.2.2. Encode categorical features.\n",
    "    # Converts other categories into 'Others'.\n",
    "    top_hosts = x_train['host'].value_counts().nlargest(n_hosts).index\n",
    "    x_train['host'] = x_train['host'].apply(lambda x: x if x in top_hosts else 'Others')\n",
    "\n",
    "    # Encode `category` and `host`.\n",
    "    categorical_features = ['category', 'host']\n",
    "    one_enc     = OneHotEncoder(handle_unknown='ignore')    # Zero vector for unknown category.\n",
    "    x_enc       = one_enc.fit_transform(x_train[categorical_features])\n",
    "\n",
    "    # Convert back to DataFrame.\n",
    "    enc_columns = one_enc.get_feature_names_out(categorical_features)\n",
    "    x_enc_df    = pd.DataFrame(x_enc.toarray(), columns=enc_columns, index=x_train.index)\n",
    "    x_train     = x_train.drop(columns=categorical_features)   # Drop original 'category' and 'host' columns.\n",
    "    x_train     = pd.concat([x_train, x_enc_df], axis=1)       # Concatenate the encoded columns back.\n",
    "\n",
    "    # 2.2.3. Merge txt columns.\n",
    "    cols_txt = ['question_title', 'question_body', 'answer']\n",
    "    x_train['txt_merged'] = x_train[cols_txt].apply(lambda row: ' '.join(row), axis=1)\n",
    "    x_train  = x_train.drop(columns=cols_txt)    # Drop original txt cols.\n",
    "    \n",
    "    # Return.\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8265f40f-e37a-4a0d-9435-372a2d94b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_CULTURE                                                                    0.0\n",
       "category_LIFE_ARTS                                                                  0.0\n",
       "category_SCIENCE                                                                    0.0\n",
       "category_STACKOVERFLOW                                                              0.0\n",
       "category_TECHNOLOGY                                                                 1.0\n",
       "host_Others                                                                         0.0\n",
       "host_askubuntu.com                                                                  0.0\n",
       "host_electronics.stackexchange.com                                                  0.0\n",
       "host_english.stackexchange.com                                                      0.0\n",
       "host_math.stackexchange.com                                                         0.0\n",
       "host_physics.stackexchange.com                                                      0.0\n",
       "host_rpg.stackexchange.com                                                          0.0\n",
       "host_serverfault.com                                                                1.0\n",
       "host_stackoverflow.com                                                              0.0\n",
       "host_superuser.com                                                                  0.0\n",
       "host_tex.stackexchange.com                                                          0.0\n",
       "txt_merged                            Get raw output from web server For research I ...\n",
       "Name: 861, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data are preprocessed!\n",
    "x_train_prep = preprocess(x_train, n_hosts=10)\n",
    "x_test_prep  = preprocess(x_test, n_hosts=10)\n",
    "x_train_prep.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac8e55-0336-400e-b772-beb9e418ae29",
   "metadata": {},
   "source": [
    "# 3. Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5d425-6c94-48c9-89ab-42106b35dc40",
   "metadata": {},
   "source": [
    "## 3.1. Choose Pretrained Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f6b3b0-7b00-412d-8eea-a39300f7f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {'distilbert' : 'distilbert-base-uncased',\n",
    "               'bert' : 'bert-base-uncased',\n",
    "               'roberta' : 'roberta-base'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbfc31-7329-4bbf-8fe2-8ea8700e20bd",
   "metadata": {},
   "source": [
    "## 3.2. Maximum Number of Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ccecd17-5521-4460-9fd0-87bc0c6a0ea3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 2000.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init data.\n",
    "x_train_tokenized = x_train_prep.copy()\n",
    "x_test_tokenized  = x_test_prep.copy()\n",
    "\n",
    "# Create tokenizer.\n",
    "from transformers import AutoTokenizer\n",
    "checkpoint = checkpoints['distilbert']\n",
    "tokenizer  = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# First, tokenize without padding and truncation.\n",
    "tokenized = tokenizer(\n",
    "    list(x_train_tokenized['txt_merged']), \n",
    "    padding=False,\n",
    "    truncation=False,  \n",
    "    return_tensors=\"np\"  \n",
    ")\n",
    "\n",
    "# Histogram - Token Length.\n",
    "x_train_tokenized['input_ids']    = list(tokenized['input_ids'])\n",
    "x_train_tokenized['token_length'] = x_train_tokenized['input_ids'].apply(len)\n",
    "x_train_tokenized['token_length'].hist(bins=200).set_xlim(0, 2000)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0814bd9-37d0-4d2d-a91e-879a7599844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length=128 covers 6.97% of samples!\n",
      "max_length=256 covers 34.51% of samples!\n",
      "max_length=512 covers 73.39% of samples!\n"
     ]
    }
   ],
   "source": [
    "# How many tokens are covered, i.e. not truncated, with the given length?\n",
    "n_train       = len(x_train_tokenized['token_length'])\n",
    "token_lengths = [128, 256, 512]   \n",
    "\n",
    "for token_length in token_lengths:\n",
    "    rank         = (x_train_tokenized['token_length'] <= token_length).sum()\n",
    "    quantile     = (rank / n_train) * 100\n",
    "    print(f\"max_length={token_length} covers {quantile:.2f}% of samples!\")\n",
    "\n",
    "# Final value for max_length = 512, which is the maximum num of tokens for many LLMs.\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf477b-30e6-4cfc-84a0-0aa6ed507864",
   "metadata": {},
   "source": [
    "## 3.3. Tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92fef5a0-5c81-4220-ad7c-d775bbed879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple method for tokenization.\n",
    "def tokenize(df):  \n",
    "    # Define Tokenizer.                   \n",
    "    tokenized = tokenizer(\n",
    "        list(df['txt_merged']),\n",
    "        padding          = True,\n",
    "        truncation       = True,\n",
    "        max_length       = max_length,\n",
    "#       stride           = 0,                 # Can be kept if you want overlapping tokens.\n",
    "        return_tensors   = \"np\"  \n",
    "    )\n",
    "\n",
    "    # Tokenize.\n",
    "    df['input_ids']      = list(tokenized['input_ids'])\n",
    "    df['attention_mask'] = list(tokenized['attention_mask'])\n",
    "    \n",
    "    df = df.drop(columns=['txt_merged'])      # Drop original text column.\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Tokenize.\n",
    "x_train_tokenized = tokenize(x_train_prep)\n",
    "x_test_tokenized  = tokenize(x_test_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b7022-10f4-43b7-85e2-9e58778d6889",
   "metadata": {},
   "source": [
    "> #### Note) `stride` for truncated tokens.  \n",
    "> As you can see, 73.39% of tokens will be truncated even with `max_length=512`.  \n",
    "> `max_length=512` is the most popular size for pretrained LLMs, so it's probably not enough just considering other LLMs.  \n",
    "> Consider `stride` instead. It's omitted in this project because of resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d24901-8fbb-4ad6-85f8-d6fcc45709e7",
   "metadata": {},
   "source": [
    "# 4. Model.\n",
    "- **Transformer** : takes input_ids and attention_mask => outputs representation.\n",
    "- **Head** : takes representation from transformer AND other features (category and host) => outputs values of 30 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28693416-8432-4495-86fc-14e09fc62675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels, additional_feature_dim):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained transformer.\n",
    "        self.transformer = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "        # Expose the transformer's config.\n",
    "        self.config = self.transformer.config\n",
    "        \n",
    "        # Combine transformer outputs with additional features\n",
    "        transformer_hidden_size = self.transformer.config.hidden_size\n",
    "        self.fc1 = nn.Linear(transformer_hidden_size + additional_feature_dim, num_labels)\n",
    "        \n",
    "#       self.fc2 = nn.Linear(256, num_labels)   # For complex Head.\n",
    "#       self.dropout = nn.Dropout(0.1)          # For dropout.\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, additional_features):\n",
    "        # Transformer output.\n",
    "        transformer_output = self.transformer(\n",
    "            input_ids      = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for concatenation.\n",
    "        cls_output     = transformer_output.last_hidden_state[:, 0, :]\n",
    "        combined_input = torch.cat([cls_output, additional_features], dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "#       x = self.dropout(torch.relu(self.fc1(combined_input)))\n",
    "        output = self.fc1(combined_input)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab22460-aaa5-4d2e-bac3-53ed7afaf350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HF Wrapper for Custom Model.\n",
    "class HuggingFaceModelWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model         # Custom model.\n",
    "        self.config     = base_model.config  # Expose the base model's config.\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, additional_features, labels=None):\n",
    "        # Forward pass through the base model.\n",
    "        output = self.base_model(input_ids           = input_ids, \n",
    "                                 attention_mask      = attention_mask, \n",
    "                                 additional_features = additional_features)\n",
    "        \n",
    "        # If labels are provided, calculate loss.\n",
    "        logits = output\n",
    "        loss   = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss    = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        # Delegate to the base model.\n",
    "        return self.base_model.prepare_inputs_for_generation(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79ed46-dd2c-480c-b717-35b9ae16c663",
   "metadata": {},
   "source": [
    "# 5. Fine-Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9310d-8589-4180-8a64-fca860410971",
   "metadata": {},
   "source": [
    "## 5.1. Evaluation Metrics : Spearman's Rank Correlation Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "606a8c02-68c4-40fc-835b-f072b4718ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions    = np.argmax(logits, axis=1) if logits.ndim == 3 else logits\n",
    "\n",
    "    # Calculate Spearman's correlation for each label.\n",
    "    spearman_corrs = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        corr, _ = spearmanr(predictions[:, i], labels[:, i])\n",
    "        spearman_corrs.append(corr)\n",
    "\n",
    "    # Return the mean of Spearman's correlation.\n",
    "    mean_spearman = np.nanmean(spearman_corrs)  # Handle NaNs if any.\n",
    "    return {\"spearman\": mean_spearman}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673204ee-bab3-4e59-b804-ab1dfb5216a8",
   "metadata": {},
   "source": [
    "## 5.2. Convert Dataset, `df` -> `ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a982d8eb-f466-494a-8b61-8515ec4b81c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_data(df, labels):\n",
    "    return {\n",
    "        \"input_ids\"             : list(df['input_ids']),\n",
    "        \"attention_mask\"        : list(df['attention_mask']),\n",
    "        \"additional_features\"   : df.iloc[:, :-2].values.tolist(),  \n",
    "        \"labels\"                : labels.to_numpy().tolist()        \n",
    "    }\n",
    "\n",
    "# Convert train and test datasets\n",
    "train_dataset = Dataset.from_dict(preprocess_data(x_train_tokenized, y_train))\n",
    "test_dataset  = Dataset.from_dict(preprocess_data(x_test_tokenized, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca04f6-9647-4579-a97c-7821bc40566d",
   "metadata": {},
   "source": [
    "## 5.3. Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c05f874-acee-4085-a006-9e2a886fb50d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model.\n",
    "checkpoint = checkpoints['bert']     # Other candidates : 'distilbert', 'roberta'.\n",
    "\n",
    "num_labels              = 30\n",
    "additional_features_dim = len(x_train_tokenized.columns) - 2    # Except 'input_ids' and 'attention_mask'.\n",
    "\n",
    "model = HuggingFaceModelWrapper(\n",
    "    base_model=CustomModel(checkpoint, num_labels, additional_features_dim))\n",
    "\n",
    "# Define Callbacks.\n",
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience  = 3,     # Stop after this consecutive non-improving eval steps\n",
    "    early_stopping_threshold = 1e-5   # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Define TrainingArguments.\n",
    "from transformers import AutoModel, TrainingArguments, Trainer\n",
    "\n",
    "# Hyperparameters.\n",
    "batch_size    = 8\n",
    "gra_steps     = 1\n",
    "eval_steps    = 100\n",
    "warmup_steps  = 0\n",
    "logging_steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                   # Directory for saving model checkpoints\n",
    "    overwrite_output_dir=True,                # Training from scratch, not from last training\n",
    "    optim=\"adamw_bnb_8bit\",                   # 8-bits Quantization of Optimizer.\n",
    "    eval_strategy=\"steps\",                    # Evaluate every few steps\n",
    "    eval_steps=eval_steps,                    # Evaluation interval\n",
    "    logging_dir=\"./logs\",                     # Directory for TensorBoard logs\n",
    "    logging_steps=logging_steps,              # Logging interval\n",
    "    per_device_train_batch_size=batch_size,   # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size,    # Batch size for evaluation\n",
    "    gradient_accumulation_steps=gra_steps,    # Steps for gradient accumulation\n",
    "    lr_scheduler_type=\"linear\",               # Learning scheduling: \"linear\"\n",
    "    warmup_steps=warmup_steps,                # Warmup steps: 150. \n",
    "    weight_decay=2e-2,                        # Weight decay\n",
    "    save_strategy=\"steps\",                    # Save model checkpoints periodically\n",
    "    save_steps=500,                           # Save every 500 steps\n",
    "    save_total_limit=3,                       # Keep the last 3 checkpoints\n",
    "    fp16=True,                                # Enable mixed precision (if supported)\n",
    "    load_best_model_at_end=True,              # Load the best model after training\n",
    "    metric_for_best_model=\"eval_spearman\",    # Use Spearman for metric for this competition.\n",
    "    greater_is_better=True,                   # Greater is better for Spearman.\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bb6e5c7a-d7ac-4a3f-9319-f7d6b86f6953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1700' max='6080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1700/6080 05:59 < 15:26, 4.73 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.426054</td>\n",
       "      <td>0.078708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.420127</td>\n",
       "      <td>0.135391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.423812</td>\n",
       "      <td>0.147738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.417900</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>0.192561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.408477</td>\n",
       "      <td>0.210495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.413362</td>\n",
       "      <td>0.208077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.406422</td>\n",
       "      <td>0.214082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.412028</td>\n",
       "      <td>0.217802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.405539</td>\n",
       "      <td>0.221833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.403151</td>\n",
       "      <td>0.228792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.402885</td>\n",
       "      <td>0.234114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>0.402333</td>\n",
       "      <td>0.234791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.399900</td>\n",
       "      <td>0.403345</td>\n",
       "      <td>0.230079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.400200</td>\n",
       "      <td>0.401032</td>\n",
       "      <td>0.246033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.401888</td>\n",
       "      <td>0.237551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.402245</td>\n",
       "      <td>0.237874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.402996</td>\n",
       "      <td>0.244298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./results\\checkpoint-1400\\pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='366' max='12160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  366/12160 00:34 < 18:35, 10.58 it/s, Epoch 0.60/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>0.399462</td>\n",
       "      <td>0.247460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.395700</td>\n",
       "      <td>0.398877</td>\n",
       "      <td>0.250791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.398543</td>\n",
       "      <td>0.252654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:31\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\transformers\\trainer.py:2473\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2471\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2472\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2473\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2474\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2475\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\transformers\\trainer.py:5130\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5129\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5130\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5131\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5132\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\accelerate\\data_loader.py:563\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m--> 563\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:759\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m--> 759\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pin_memory_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:75\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# The sequence type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new sequence.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the sequence type is mutable.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)\n\u001b[0;32m     74\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m---> 75\u001b[0m         {k: pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m k, sample \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:75\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# The sequence type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new sequence.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the sequence type is mutable.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)\n\u001b[0;32m     74\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m---> 75\u001b[0m         {k: \u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, sample \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:64\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpin_memory\u001b[39m(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define Trainer and train.\n",
    "\n",
    "# Full-Training.\n",
    "training_args.num_train_epochs = 10\n",
    "training_args.learning_rate    = 4e-5\n",
    "trainer_full = Trainer(\n",
    "    model            = model,                          \n",
    "    args             = training_args,              # TrainingArguments.\n",
    "    train_dataset    = train_dataset,              # Training dataset.\n",
    "    eval_dataset     = test_dataset,               # Validation dataset.\n",
    "    processing_class = tokenizer,                  # Tokenizer.\n",
    "    compute_metrics  = compute_metrics,            # Spearsman.\n",
    "    callbacks        = [early_stopping_callback]   # EarlyStopping callback.\n",
    ")\n",
    "trainer_full.train()\n",
    "\n",
    "# Freeze Body.\n",
    "for param in model.base_model.transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Head-only Training.\n",
    "training_args.num_train_epochs = 20\n",
    "training_args.learning_rate    = 3e-5\n",
    "trainer_head = Trainer(\n",
    "    model=model,                          \n",
    "    args=training_args,                   # TrainingArguments\n",
    "    train_dataset=train_dataset,          # Training dataset\n",
    "    eval_dataset=test_dataset,            # Validation dataset\n",
    "    processing_class=tokenizer,           # Tokenizer\n",
    "    compute_metrics=compute_metrics,      # Evaluation metric function\n",
    "    callbacks=[early_stopping_callback]   # EarlyStopping Callback.\n",
    ")\n",
    "trainer_head.train()\n",
    "\n",
    "# Notification for finish.\n",
    "# import winsound\n",
    "# winsound.PlaySound(\"Alarm03\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f2ac0e-19e7-4241-b99b-59c1458e5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rank.\n",
    "history  = trainer_head.state.log_history   # Check `trainer_head` or `trainer_full`.\n",
    "my_score = max(\n",
    "    log[\"eval_spearman\"] for log in history if \"eval_spearman\" in log\n",
    ")\n",
    "check_rank(my_score)\n",
    "\n",
    "plot_learning_curve(trainer_full.state.log_history)\n",
    "plot_learning_curve(trainer_head.state.log_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6ba30-c100-48fc-a5fe-26171e8af955",
   "metadata": {},
   "source": [
    "### DistilBERT.\n",
    "| Parameter                 | Value                   |\n",
    "|---------------------------|-------------------------|\n",
    "| Model                     | distilbert-base-uncased |\n",
    "| Max Length                | 512                     |\n",
    "| Learning Rate (Full)      | 4e-5                    |\n",
    "| Learning Rate (Head)      | 3e-5                    |\n",
    "| Weight Decay              | 2e-2                    |\n",
    "| Warmup Steps              | 0                       |\n",
    "| Batch Size                | 8                       |\n",
    "| Gradient Accumulation     | 1                       |\n",
    "| Evaluation Steps          | 100                     |\n",
    "| Early Stopping Patience   | 3                       |\n",
    "| Early Stopping Threshold  | 1e-5                    |\n",
    "| Full Fine-Tuning Epochs   | 10      |\n",
    "| Head Fine-Tuning Epochs   | 20       |\n",
    "| Spearman Correlation      | 0.3783                  |\n",
    "| CPU Time                  | 7min 11s               |\n",
    "| Wall Time                 | 5min 50s               |\n",
    "| Train Runtime             | 111.5316 seconds        |\n",
    "| Train Samples Per Second  | 872.04                 |\n",
    "| Train Steps Per Second    | 109.027                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447fb7a5-56b0-46b7-bf96-4009ae199e42",
   "metadata": {},
   "source": [
    "### BERT.\n",
    "\n",
    "| Parameter                   | Value                   |\n",
    "|-----------------------------|-------------------------|\n",
    "| Model                       | bert-base-uncased       |\n",
    "| Max Length                  | 512                     |\n",
    "| Learning Rate (Full)        | 4e-5                    |\n",
    "| Learning Rate (Head)        | 3e-5                    |\n",
    "| Weight Decay                | 2e-2                    |\n",
    "| Warmup Steps                | 0                       |\n",
    "| Batch Size                  | 8                       |\n",
    "| Gradient Accumulation Steps | 1                       |\n",
    "| Evaluation Steps            | 100                     |\n",
    "| Early Stopping Patience     | 3                       |\n",
    "| Early Stopping Threshold    | 1e-5                    |\n",
    "| Full Fine-Tuning Epochs     | 10      |\n",
    "| Head Fine-Tuning Epochs     | 20                      |\n",
    "| Spearman Correlation        | 0.3813 (127 / 1572)     |\n",
    "| CPU Time                    | 10min 32s              |\n",
    "| Wall Time                   | 9min 26s               |\n",
    "| Train Runtime               | 125.9416 seconds        |\n",
    "| Train Samples Per Second    | 772.262                |\n",
    "| Train Steps Per Second      | 96.553                 |\n",
    "| Training Loss               | 0.324978                |\n",
    "| Global Steps                | 1200                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f000a-1090-4e03-935c-628342804437",
   "metadata": {},
   "source": [
    "# 6. Summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4ddbb-1c4d-499b-b828-1e54612d0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load.\n",
    "from sklearn.model_selection import train_test_split\n",
    "data                = pd.read_csv('./dataset/train.csv')\n",
    "train_set, test_set = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train-Test Split.\n",
    "label_col_idx = train_set.columns.get_loc('question_asker_intent_understanding')  # Label Columns start from it.\n",
    "x_train       = train_set.iloc[:, :label_col_idx]\n",
    "y_train       = train_set.iloc[:, label_col_idx:]\n",
    "x_test        = test_set.iloc[:, :label_col_idx]\n",
    "y_test        = test_set.iloc[:, label_col_idx:]\n",
    "\n",
    "# Preprocessing.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess(x_train, n_hosts=10):\n",
    "    # 2.2.1. Drop redundant features.\n",
    "    redundant_features   = ['qa_id', 'question_user_name', 'question_user_page', \n",
    "                            'answer_user_page', 'answer_user_name', 'url']\n",
    "    x_train = x_train.drop(columns=redundant_features)\n",
    "\n",
    "    # 2.2.2. Encode categorical features.\n",
    "    # Converts other categories into 'Others'.\n",
    "    top_hosts = x_train['host'].value_counts().nlargest(n_hosts).index\n",
    "    x_train['host'] = x_train['host'].apply(lambda x: x if x in top_hosts else 'Others')\n",
    "\n",
    "    # Encode `category` and `host`.\n",
    "    categorical_features = ['category', 'host']\n",
    "    one_enc     = OneHotEncoder(handle_unknown='ignore')    # Zero vector for unknown category.\n",
    "    x_enc       = one_enc.fit_transform(x_train[categorical_features])\n",
    "\n",
    "    # Convert back to DataFrame.\n",
    "    enc_columns = one_enc.get_feature_names_out(categorical_features)\n",
    "    x_enc_df    = pd.DataFrame(x_enc.toarray(), columns=enc_columns, index=x_train.index)\n",
    "    x_train     = x_train.drop(columns=categorical_features)   # Drop original 'category' and 'host' columns.\n",
    "    x_train     = pd.concat([x_train, x_enc_df], axis=1)       # Concatenate the encoded columns back.\n",
    "\n",
    "    # 2.2.3. Merge txt columns.\n",
    "    cols_txt = ['question_title', 'question_body', 'answer']\n",
    "    x_train['txt_merged'] = x_train[cols_txt].apply(lambda row: ' '.join(row), axis=1)\n",
    "    x_train  = x_train.drop(columns=cols_txt)    # Drop original txt cols.\n",
    "    \n",
    "    # Return.\n",
    "    return x_train\n",
    "\n",
    "x_train_prep = preprocess(x_train, n_hosts=10)\n",
    "x_test_prep  = preprocess(x_test, n_hosts=10)\n",
    "\n",
    "# Tokenize.\n",
    "def tokenize(df):  \n",
    "    # Define Tokenizer.                   \n",
    "    tokenized = tokenizer(\n",
    "        list(df['txt_merged']),\n",
    "        padding          = True,\n",
    "        truncation       = True,\n",
    "        max_length       = max_length,\n",
    "#       stride           = 0,                 # Can be kept if you want overlapping tokens.\n",
    "        return_tensors   = \"np\"  \n",
    "    )\n",
    "\n",
    "    df['input_ids']      = list(tokenized['input_ids'])\n",
    "    df['attention_mask'] = list(tokenized['attention_mask'])\n",
    "    \n",
    "    df = df.drop(columns=['txt_merged'])      # Drop original text column.\n",
    "    \n",
    "    return df\n",
    "\n",
    "x_train_tokenized = tokenize(x_train_prep)\n",
    "x_test_tokenized  = tokenize(x_test_prep)\n",
    "\n",
    "# Convert datasets, `df` -> `ds`.\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(preprocess_data(x_train_tokenized, y_train))\n",
    "test_dataset  = Dataset.from_dict(preprocess_data(x_test_tokenized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb3905-d9e9-4fb2-837e-0abaf28752ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.\n",
    "\n",
    "# Initialize the model.\n",
    "checkpoint = checkpoints['bert']     # Other candidates : 'distilbert', 'roberta'.\n",
    "\n",
    "num_labels              = 30\n",
    "additional_features_dim = len(x_train_tokenized.columns) - 2    # Except 'input_ids' and 'attention_mask'.\n",
    "\n",
    "model = HuggingFaceModelWrapper(\n",
    "    base_model=CustomModel(checkpoint, num_labels, additional_features_dim))\n",
    "\n",
    "# Define Callbacks.\n",
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience  = 3,     # Stop after this consecutive non-improving eval steps\n",
    "    early_stopping_threshold = 1e-5   # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Define TrainingArguments.\n",
    "from transformers import AutoModel, TrainingArguments, Trainer\n",
    "\n",
    "# Hyperparameters.\n",
    "batch_size    = 8\n",
    "gra_steps     = 1\n",
    "eval_steps    = 100\n",
    "warmup_steps  = 0\n",
    "logging_steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                   # Directory for saving model checkpoints\n",
    "    overwrite_output_dir=True,                # Training from scratch, not from last training\n",
    "    optim=\"adamw_bnb_8bit\",                   # 8-bits Quantization of Optimizer.\n",
    "    eval_strategy=\"steps\",                    # Evaluate every few steps\n",
    "    eval_steps=eval_steps,                    # Evaluation interval\n",
    "    logging_dir=\"./logs\",                     # Directory for TensorBoard logs\n",
    "    logging_steps=logging_steps,              # Logging interval\n",
    "    per_device_train_batch_size=batch_size,   # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size,    # Batch size for evaluation\n",
    "    gradient_accumulation_steps=gra_steps,    # Steps for gradient accumulation\n",
    "    lr_scheduler_type=\"linear\",               # Learning scheduling: \"linear\"\n",
    "    warmup_steps=warmup_steps,                # Warmup steps: 150. \n",
    "    weight_decay=2e-2,                        # Weight decay\n",
    "    save_strategy=\"steps\",                    # Save model checkpoints periodically\n",
    "    save_steps=500,                           # Save every 500 steps\n",
    "    save_total_limit=3,                       # Keep the last 3 checkpoints\n",
    "    fp16=True,                                # Enable mixed precision (if supported)\n",
    "    load_best_model_at_end=True,              # Load the best model after training\n",
    "    metric_for_best_model=\"eval_spearman\",    # Use Spearman for metric for this competition.\n",
    "    greater_is_better=True,                   # Greater is better for Spearman.\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af863bb6-883e-4ffc-81c0-78cd5e5220a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define Trainer and train.\n",
    "\n",
    "# Full-Training.\n",
    "training_args.num_train_epochs = 10\n",
    "training_args.learning_rate    = 4e-5\n",
    "trainer_full = Trainer(\n",
    "    model            = model,                          \n",
    "    args             = training_args,              # TrainingArguments.\n",
    "    train_dataset    = train_dataset,              # Training dataset.\n",
    "    eval_dataset     = test_dataset,               # Validation dataset.\n",
    "    processing_class = tokenizer,                  # Tokenizer.\n",
    "    compute_metrics  = compute_metrics,            # Spearsman.\n",
    "    callbacks        = [early_stopping_callback]   # EarlyStopping callback.\n",
    ")\n",
    "trainer_full.train()\n",
    "\n",
    "# Freeze Body.\n",
    "for param in model.base_model.transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Head-only Training.\n",
    "training_args.num_train_epochs = 20\n",
    "training_args.learning_rate    = 3e-5\n",
    "trainer_head = Trainer(\n",
    "    model=model,                          \n",
    "    args=training_args,                   # TrainingArguments\n",
    "    train_dataset=train_dataset,          # Training dataset\n",
    "    eval_dataset=test_dataset,            # Validation dataset\n",
    "    processing_class=tokenizer,           # Tokenizer\n",
    "    compute_metrics=compute_metrics,      # Evaluation metric function\n",
    "    callbacks=[early_stopping_callback]   # EarlyStopping Callback.\n",
    ")\n",
    "trainer_head.train()\n",
    "\n",
    "# Notification for finish.\n",
    "# import winsound\n",
    "# winsound.PlaySound(\"Alarm03\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fac08a-9e79-444d-b536-052daaa0bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines.\n",
    "\n",
    "# Define Custom Model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels, additional_feature_dim):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained transformer.\n",
    "        self.transformer = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "        # Expose the transformer's config.\n",
    "        self.config = self.transformer.config\n",
    "        \n",
    "        # Combine transformer outputs with additional features\n",
    "        transformer_hidden_size = self.transformer.config.hidden_size\n",
    "        self.fc1 = nn.Linear(transformer_hidden_size + additional_feature_dim, num_labels)\n",
    "        \n",
    "#       self.fc2 = nn.Linear(256, num_labels)   # For complex Head.\n",
    "#       self.dropout = nn.Dropout(0.1)          # For dropout.\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, additional_features):\n",
    "        # Transformer output.\n",
    "        transformer_output = self.transformer(\n",
    "            input_ids      = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for concatenation.\n",
    "        cls_output     = transformer_output.last_hidden_state[:, 0, :]\n",
    "        combined_input = torch.cat([cls_output, additional_features], dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "#       x = self.dropout(torch.relu(self.fc1(combined_input)))\n",
    "        output = self.fc1(combined_input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Define HF Wrapper for Custom Model.\n",
    "class HuggingFaceModelWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model         # Custom model.\n",
    "        self.config     = base_model.config  # Expose the base model's config.\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, additional_features, labels=None):\n",
    "        # Forward pass through the base model.\n",
    "        output = self.base_model(input_ids           = input_ids, \n",
    "                                 attention_mask      = attention_mask, \n",
    "                                 additional_features = additional_features)\n",
    "        \n",
    "        # If labels are provided, calculate loss.\n",
    "        logits = output\n",
    "        loss   = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss    = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        # Delegate to the base model.\n",
    "        return self.base_model.prepare_inputs_for_generation(*args, **kwargs)\n",
    "\n",
    "# Spearman's Corr.\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions    = np.argmax(logits, axis=1) if logits.ndim == 3 else logits\n",
    "\n",
    "    # Calculate Spearman's correlation for each label.\n",
    "    spearman_corrs = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        corr, _ = spearmanr(predictions[:, i], labels[:, i])\n",
    "        spearman_corrs.append(corr)\n",
    "\n",
    "    # Return the mean of Spearman's correlation.\n",
    "    mean_spearman = np.nanmean(spearman_corrs)  # Handle NaNs if any.\n",
    "    return {\"spearman\": mean_spearman}\n",
    "\n",
    "# Convert datasets, `df` -> `ds`.\n",
    "def preprocess_data(df, labels):\n",
    "    return {\n",
    "        \"input_ids\"             : list(df['input_ids']),\n",
    "        \"attention_mask\"        : list(df['attention_mask']),\n",
    "        \"additional_features\"   : df.iloc[:, :-2].values.tolist(),  \n",
    "        \"labels\"                : labels.to_numpy().tolist()        \n",
    "    }\n",
    "\n",
    "# Method to Check Rank.\n",
    "def check_rank(score):\n",
    "    leaderboard = pd.read_csv('./leaderboard.csv')\n",
    "    num_team    = len(leaderboard)\n",
    "    mean        = leaderboard['Score'].mean()\n",
    "    median      = leaderboard['Score'].median()\n",
    "\n",
    "    my_rank = (leaderboard['Score'] >= my_score).sum()\n",
    "\n",
    "    print(f'My Rank = {my_rank} / {num_team}')\n",
    "    print(f'My Score = {score:.4f}')\n",
    "    print(f'Mean = {mean:.4f}')\n",
    "    print(f'Median = {median:.4f}')\n",
    "\n",
    "# Plot Learning Curve.\n",
    "def plot_learning_curve(history):\n",
    "    eval_loss  = [log[\"eval_loss\"] for log in history if \"eval_loss\" in log]\n",
    "    train_loss = [log[\"loss\"] for log in history if \"loss\" in log]\n",
    "    spear_loss = [log[\"eval_spearman\"] for log in history if \"loss\" in log]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Training Loss curve\n",
    "    plt.plot(\n",
    "        [log[\"step\"] for log in history if \"loss\" in log],\n",
    "        train_loss,\n",
    "        label=\"Training Loss\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    \n",
    "    # Validation Loss curve\n",
    "    plt.plot(\n",
    "        [log[\"step\"] for log in history if \"eval_loss\" in log],\n",
    "        eval_loss,\n",
    "        label=\"Validation Loss\",\n",
    "        marker=\"x\",\n",
    "    )\n",
    "\n",
    "    # Spearman Loss curve\n",
    "    plt.plot(\n",
    "        [log[\"step\"] for log in history if \"eval_loss\" in log],\n",
    "        spear_loss,\n",
    "        label=\"Spearman Loss\",\n",
    "        marker=\"x\",\n",
    "    )\n",
    "    \n",
    "    # Labels and legends\n",
    "    plt.title(\"Training and Validation Loss Curve\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
