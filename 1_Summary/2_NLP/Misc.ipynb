{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c123b33a-600b-4bda-975b-714e1b1a7202",
   "metadata": {},
   "source": [
    "# Emoji.\n",
    "- ðŸŽ‰\n",
    "- ðŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1b05d-0b4e-477a-97e8-1fa3fc3c7aa6",
   "metadata": {},
   "source": [
    "# Terminology.\n",
    "- Logit : A raw output from the model, before processed with postprocessings like activation layer.\n",
    "- Token : A piece of text that is splited from a sentence by tokenizer.\n",
    "- Causal LM : Predicts next token.\n",
    "- Masked LM : Predicts a mask in the sentence.\n",
    "- Encoding : raw txt -> numbers.\n",
    "- Decoding : numbers -> raw txt.\n",
    "- Dynamic Padding : After making a batch, find the longest sample and pad last of samples with this length.\n",
    "- Collate Function : Responsible for putting together samples inside a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7432216-5a10-40ff-8b1c-55b9aef955d6",
   "metadata": {},
   "source": [
    "## Ignore warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73638b6-5b90-4a9d-8b74-dd65ebb6871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all warnings.\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# HF warning for symlinks.\n",
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338b97b-2740-480f-bba5-95fbb81757d3",
   "metadata": {},
   "source": [
    "## Measuring an execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791c90c-2942-4fe7-b8c7-0ec26ae8f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time print('1 line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5380889-70b8-4016-8b42-ef9496e25a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole cell\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "print('whole cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2d6ec-d50e-4a9a-a252-123f355c6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# print('iterative')  # DO NOT RUN THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df902242-c47b-4a1a-9ed9-742e4c67563e",
   "metadata": {},
   "source": [
    "## Memory Usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e109508-8a3a-4212-a6af-1ba2e998734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAN used : 65.48 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f'RAN used : {psutil.Process().memory_info().rss / (1024**2):.2f} MB')   # Resident set size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9220cf-3abf-4919-85ab-f2c3323515f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### General Guide to Choosing Batch Size for Model Training\n",
    "\n",
    "1. **Understand the Memory Usage**:\n",
    "   - Each sample consists of tokenized data, where each token is represented as an integer (typically 4 bytes in PyTorch).\n",
    "   - Estimate the average number of tokens per sample by analyzing your dataset.\n",
    "\n",
    "2. **Account for Model and Gradient Memory**:\n",
    "   - Reserve VRAM for the model parameters, gradients, and other computations.\n",
    "   - Larger models (e.g., ALBERT) require more memory for gradients (~2 GB for ALBERT `base`).\n",
    "\n",
    "3. **Calculate Maximum Batch Size**:\n",
    "   - Divide available memory for data by the estimated memory per sample.\n",
    "   - Apply a safety margin to account for padding, intermediate computations, and possible memory spikes.\n",
    "\n",
    "4. **Optimize for Efficiency**:\n",
    "   - Choose a batch size as a power of 2 (e.g., 128, 256, 512) for better GPU utilization and compatibility with mixed precision training.\n",
    "   - If not using such optimizations, any batch size close to the calculated limit is fine.\n",
    "\n",
    "5. **Monitor and Adjust**:\n",
    "   - Check GPU memory usage during training. If you encounter OOM (Out Of Memory) errors, reduce the batch size.\n",
    "   - Use tools like NVIDIA `nvidia-smi` or PyTorchâ€™s `torch.cuda.memory_allocated()` to monitor VRAM usage.\n",
    "\n",
    "### Summary:\n",
    "- Batch size depends on dataset size, tokenization, model architecture, and available GPU VRAM.\n",
    "- Use powers of 2 (e.g., 256, 512) for better efficiency.\n",
    "- Start with a conservative estimate and adjust based on memory usage.\n",
    "\n",
    "### Example (IMDb Dataset with ALBERT on RTX 3070):\n",
    "1. **Memory Usage**:\n",
    "   - Average character length: 1,325.\n",
    "   - Approximate tokens per sample: \\(1,325 \\times 0.6 \\approx 795\\).\n",
    "   - Memory per sample: \\(795 \\times 4 \\, \\text{bytes} \\approx 3.2 \\, \\text{KB}\\).\n",
    "\n",
    "2. **Available Memory**:\n",
    "   - GPU VRAM: 8 GB.\n",
    "   - Reserve ~2 GB for model and gradients, leaving ~6 GB for data.\n",
    "\n",
    "3. **Batch Size Calculation**:\n",
    "   - Maximum batch size: \\(6,000,000 \\, \\text{KB} / 3.2 \\, \\text{KB per sample} \\approx 1,875\\).\n",
    "   - Safe batch size (with 50% margin): \\(1,875 / 2 \\approx 900\\).\n",
    "\n",
    "4. **Final Recommendation**:\n",
    "   - Use a batch size of **512** or **768** for efficient training.\n",
    "   - Adjust further based on memory usage and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38d611-5b82-4d4e-9299-520fe4504de3",
   "metadata": {},
   "source": [
    "## Random Seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ee6fc-2aa6-472f-9e2d-5a67b60fdb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "import tensorflow as tf\n",
    "\n",
    "set_seed(42)              # For HF.\n",
    "tf.random.set_seed(42)    # For tf, np, and python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5002800-1e3c-47c3-b113-05f9804555ca",
   "metadata": {},
   "source": [
    "## Stop Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926fc0d-b1fd-4deb-862c-ef5880e59566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude some frequent words that are rarely meaningful.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec05ab-e53d-4b18-abc2-2a053e3d9734",
   "metadata": {},
   "source": [
    "# Login to HF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847424c7-24c7-4cb9-a743-f65d4e270b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to hub.\n",
    "from huggingface_hub import login\n",
    "hf_token = \"your-huggingface-token\"\n",
    "login(token=hf_token)\n",
    "\n",
    "evaluate.push_to_hub(\n",
    "    model_id=\"your-username/your-repo-name\",  # Your repository\n",
    "    metric_value=0.5,\n",
    "    metric_type=\"bleu\",\n",
    "    metric_name=\"BLEU\",\n",
    "    dataset_type=\"wikitext\",\n",
    "    dataset_name=\"WikiText\",\n",
    "    dataset_split=\"test\",\n",
    "    task_type=\"text-generation\",\n",
    "    task_name=\"Text Generation\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
