{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346d4fad-7c40-4a08-b555-1acddf6eecc2",
   "metadata": {},
   "source": [
    "# 0. Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad01ff5-b3cf-41d9-b2b4-6c3263262d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517db536-260c-42ff-bed6-42d305059c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        float: left;\n",
       "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337dd06e-e9e1-45de-a000-55dd86c7ef18",
   "metadata": {},
   "source": [
    "# 1. Optuna.\n",
    "\n",
    "## 1.1. Why use `Optuna`?\n",
    "1. **Automated Hyperparameter Optimization**:\n",
    "   - Simplifies the search for optimal hyperparameters using strategies like TPE and grid search.\n",
    "2. **Efficient Search Algorithms**:\n",
    "   - Employs smarter search strategies (e.g., Bayesian optimization) for faster convergence compared to brute-force methods.\n",
    "3. **Dynamic Pruning**:\n",
    "   - Automatically stops unpromising trials early, saving time and computational resources.\n",
    "4. **Flexible Integration**:\n",
    "   - Works seamlessly with frameworks like PyTorch, TensorFlow, and Hugging Face.\n",
    "5. **Rich Visualizations**:\n",
    "   - Provides easy-to-use tools for analyzing optimization history, hyperparameter importance, and parameter interactions.\n",
    "6. **Scalable and Distributed**:\n",
    "   - Supports parallel and distributed optimization across multiple machines.\n",
    "7. **Persistent Study Management**:\n",
    "   - Allows storing study results in databases (e.g., SQLite, PostgreSQL) for reproducibility and later analysis.\n",
    "8. **Customization**:\n",
    "   - Enables users to define custom objective functions and constraints for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e6950-f6c0-46b0-94f4-dfb6703f0430",
   "metadata": {},
   "source": [
    "## 1.2. Basic Usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caa922dd-3cec-4a30-909f-9ed6cfdc94a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5a7d76a8a84460b0b2c5f7735d17c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98060163ed754281b2ef93fef48566b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:45:38,708] A new study created in memory with name: no-name-38255e0c-618a-4e56-b4eb-714e595c9adc\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\yana\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:46:52,277] Trial 0 finished with value: 0.0012700868537649512 and parameters: {'learning_rate': 1.9007819166954822e-05, 'dropout_rate': 0.42922597921929884}. Best is trial 0 with value: 0.0012700868537649512.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.003179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:48:06,139] Trial 1 finished with value: 0.003179125254973769 and parameters: {'learning_rate': 1.0149524372475638e-05, 'dropout_rate': 0.16525950829714547}. Best is trial 0 with value: 0.0012700868537649512.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.003055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:49:20,221] Trial 2 finished with value: 0.0030551033560186625 and parameters: {'learning_rate': 1.0385910639905127e-05, 'dropout_rate': 0.3422294596561477}. Best is trial 0 with value: 0.0012700868537649512.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:50:34,316] Trial 3 finished with value: 0.0002945879241451621 and parameters: {'learning_rate': 3.787297653035789e-05, 'dropout_rate': 0.23807413522898158}. Best is trial 3 with value: 0.0002945879241451621.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:51:48,473] Trial 4 finished with value: 0.0012351060286164284 and parameters: {'learning_rate': 1.7754826308156474e-05, 'dropout_rate': 0.25629158428026444}. Best is trial 3 with value: 0.0002945879241451621.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:53:02,535] Trial 5 finished with value: 0.002876021433621645 and parameters: {'learning_rate': 1.0755435438670248e-05, 'dropout_rate': 0.35170205857494086}. Best is trial 3 with value: 0.0002945879241451621.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:54:17,048] Trial 6 finished with value: 0.0006501026218757033 and parameters: {'learning_rate': 2.554368477456048e-05, 'dropout_rate': 0.2662273820819414}. Best is trial 3 with value: 0.0002945879241451621.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:55:31,326] Trial 7 finished with value: 0.0003851080546155572 and parameters: {'learning_rate': 3.3211808055202046e-05, 'dropout_rate': 0.37207385083974653}. Best is trial 3 with value: 0.0002945879241451621.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:56:45,543] Trial 8 finished with value: 0.0010834855493158102 and parameters: {'learning_rate': 1.9081663752353572e-05, 'dropout_rate': 0.37805772805827087}. Best is trial 3 with value: 0.0002945879241451621.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-29 00:57:59,911] Trial 9 finished with value: 0.0013752002269029617 and parameters: {'learning_rate': 1.6695368150555907e-05, 'dropout_rate': 0.23315557648412463}. Best is trial 3 with value: 0.0002945879241451621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 3.787297653035789e-05, 'dropout_rate': 0.23807413522898158}\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation.\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "\n",
    "# Load.\n",
    "ds       = load_dataset('imdb')\n",
    "train_ds = Dataset.from_dict(ds['train'][:2500])  \n",
    "test_ds  = Dataset.from_dict(ds['test'][:2500])   \n",
    "\n",
    "# Tokenization.\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize(ds):\n",
    "    return tokenizer(ds['text'], truncation=True, padding=True)\n",
    "\n",
    "train_tokenized = train_ds.map(tokenize, batched=True)\n",
    "test_tokenized  = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Define objective function for Optuna.\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions from Optuna.\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)   # Search in continuous range (1e-5, 5e-5).\n",
    "    dropout_rate  = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)                # [0.1, 0.5] for discrete values instead!\n",
    "\n",
    "    # Configure model directly.\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    model.classifier.dropout = nn.Dropout(dropout_rate)  # Dropout rate is tuned via Optuna.\n",
    "\n",
    "    # Training arguments.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./temp\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=learning_rate,     # Learning rate is tuned via Optuna.\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"no\"\n",
    "    )\n",
    "\n",
    "    # Trainer.\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=test_tokenized\n",
    "    )\n",
    "\n",
    "    # Train and evaluate.\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    return eval_results[\"eval_loss\"]\n",
    "\n",
    "# Run Optuna study.\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Best hyperparameters.\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82deb3e-6f01-4f07-af34-c7266f3a8822",
   "metadata": {},
   "source": [
    "## 1.3. Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81876490-5be9-4bd8-bfc6-1f98e80198d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:14] \"GET / HTTP/1.1\" 302 0\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:14] \"GET /dashboard HTTP/1.1\" 200 4145\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:15] \"GET /static/bundle.js HTTP/1.1\" 200 4158971\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:16] \"GET /api/studies HTTP/1.1\" 200 141\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:16] \"GET /favicon.ico HTTP/1.1\" 200 7670\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:20] \"GET /api/studies/1/param_importances HTTP/1.1\" 200 27\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:21] \"GET /api/studies/1?after=0 HTTP/1.1\" 200 1220\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:21] \"GET /api/meta HTTP/1.1\" 200 64\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:31] \"GET /api/studies/1?after=1 HTTP/1.1\" 200 380\n",
      "127.0.0.1 - - [29/Dec/2024 01:02:42] \"GET /api/studies/1?after=1 HTTP/1.1\" 200 380\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m IFrame(src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:8080/\u001b[39m\u001b[38;5;124m\"\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Plot visualizations\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\optuna\\visualization\\_optimization_history.py:200\u001b[0m, in \u001b[0;36mplot_optimization_history\u001b[1;34m(study, target, target_name, error_bar)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_optimization_history\u001b[39m(\n\u001b[0;32m    173\u001b[0m     study: Study \u001b[38;5;241m|\u001b[39m Sequence[Study],\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m     error_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo.Figure\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m     \u001b[43m_imports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m     info_list \u001b[38;5;241m=\u001b[39m _get_optimization_history_info_list(study, target, target_name, error_bar)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_optimization_history_plot(info_list, target_name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\optuna\\_imports.py:95\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     exc_value, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc_value\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "from optuna import create_study\n",
    "from optuna_dashboard import run_server\n",
    "from optuna.visualization import (\n",
    "    plot_param_importances,\n",
    "    plot_optimization_history,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_contour,\n",
    "    plot_slice,\n",
    "    plot_edf,\n",
    ")\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Run the integrated dashboard.\n",
    "run_server(\"sqlite:///example_study.db\", port=8080)  # Use storage path for the dashboard\n",
    "\n",
    "# Display the dashboard inline in Jupyter Notebook.\n",
    "IFrame(src=\"http://127.0.0.1:8080/\", width=1000, height=600)\n",
    "\n",
    "# Plot visualizations\n",
    "plot_optimization_history(study).show()      # Optimization history.\n",
    "plot_param_importances(study).show()         # Hyperparameter importance.\n",
    "plot_parallel_coordinate(study).show()       # Parallel coordinates plot.\n",
    "plot_contour(study).show()                   # Contour plot.\n",
    "plot_slice(study).show()                     # Slice plot.\n",
    "plot_edf(study).show()                       # Empirical Distribution Function (EDF) plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c6f204-f504-42ed-b42d-ffe72923e80f",
   "metadata": {},
   "source": [
    "# 2. PEFT.\n",
    "- Reduce the number of trainable parameters by using **Prompt Parameters** or **Reparametrization Methods** like LoRA.\n",
    "- Good in general, default for large LLM.\n",
    "- Tradeoffs of using PEFT:\n",
    "  - It may not work well with small dataset for downstream,\n",
    "  - Except LoRA, PEFT is not compatible with quantization,\n",
    "  - In multitask training, PEFT does not generalized well and increases complexity.  \n",
    "    => Consider merging adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859942f8-dfd6-4054-902d-9fe07191abb2",
   "metadata": {},
   "source": [
    "## 2.1. PeftConfig.\n",
    "- Contains all important parameters of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a297e32a-6f16-4085-a67f-7c15b3cebfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# LoRA.\n",
    "peft_config = LoraConfig(task_type       = TaskType.SEQ_2_SEQ_LM, \n",
    "                         inference_mode  = False,                   # If 'True', weights are frozen and not trained.\n",
    "                         r=8, lora_alpha =32, lora_dropout=0.1)\n",
    "# Prompt-based.\n",
    "peft_config = PromptEncoderConfig(task_type=\"CAUSAL_LM\", \n",
    "                                  num_virtual_tokens=20, \n",
    "                                  encoder_hidden_size=128)\n",
    "# IA3.\n",
    "peft_config = IA3Config(task_type=\"SEQ_2_SEQ_LM\")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767aef1-3c27-4b6d-bdac-7c24b7b6352c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "| Task Type                | Description                                                              |\n",
    "|--------------------------|--------------------------------------------------------------------------|\n",
    "| `SEQ_2_SEQ_LM`            | Sequence-to-sequence language modeling (e.g., translation, summarization)|\n",
    "| `Causal_LM`               | Causal language modeling (e.g., text generation)                        |\n",
    "| `Masked_LM`               | Masked language modeling (e.g., BERT-based models for masked token prediction)|\n",
    "| `Token_CLS`               | Token-level classification (e.g., named entity recognition)            |\n",
    "| `Seq_CLS`                 | Sequence-level classification (e.g., sentiment analysis, document classification)|\n",
    "| `MultipleChoice`          | Multiple choice tasks (e.g., question answering with multiple options) |\n",
    "| `QA`                      | Question answering tasks (e.g., extractive question answering)         |\n",
    "| `Text_Classification`     | Text classification tasks (e.g., sentiment analysis)                    |\n",
    "| `Text_2_Text_Generation`  | Text-to-text generation tasks (e.g., text summarization, translation)  |\n",
    "| `Text_Summarization`      | Summarization tasks (e.g., extractive or abstractive summarization)    |\n",
    "| `Next_Sentence_Prediction`| Predicting whether two sentences follow each other in a sequence      |\n",
    "| `Sentence_Classification` | Classification of sentence pairs (e.g., entailment, similarity)        |\n",
    "| `Token_Regression`        | Regression tasks on token-level inputs                                 |\n",
    "| `Sentence_Regression`     | Regression tasks on sentence-level inputs                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efa2e3-3fb0-4ab7-9917-bcc1e1bebed0",
   "metadata": {},
   "source": [
    "## How to Choose PEFT.\n",
    "\n",
    "| **Method**       | **When to Use**                                                               | **Best For**                                      |\n",
    "|:------------------|:-----------------------------------------------------------------------------|:-------------------------------------------------|\n",
    "| **LoRA**         | - Large pretrained models                                                    | - Task-specific fine-tuning                      |\n",
    "|                  | - Low computational resources                                                | - Memory efficiency, reduced training cost       |\n",
    "|                  | - Need for efficient fine-tuning on specific downstream tasks                | - Tasks like text classification, summarization, etc. |\n",
    "|                  | - Compatible with 4-bit and 8-bit quantization.                              |                                                  |\n",
    "|                  | - Works well with a sufficient amount of data for downstream tasks.          |                                                  |\n",
    "| **Prompt-based** | - Few-shot/zero-shot learning                                                | - Tasks where model generalization is key        |\n",
    "|                  | - Limited data, but need flexibility in task handling                        | - Multi-task learning without modifying model weights |\n",
    "| **IA3**          | - Few-shot adaptation with task-specific attention                           | - Tasks requiring individualized attention adaptation |\n",
    "|                  | - Memory efficiency with targeted model modifications                        | - Fine-tuning attention mechanisms for specific tasks |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d1f30-5c58-47a4-9241-1ba3147bfc65",
   "metadata": {},
   "source": [
    "# 2.2. PeftModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a60317-ab1d-464b-9204-a5d2b6f6fe96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63a461844ce4d88a4f104a156182585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/800 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yana\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yana\\.cache\\huggingface\\hub\\models--bigscience--mt0-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fc2f7f7c29492cb06290197085b512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Base Model.\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "checkpoint = \"bigscience/mt0-large\"\n",
    "model      = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "# Peft Model.\n",
    "from peft import get_peft_model\n",
    "model      = get_peft_model(model, peft_config)  \n",
    "\n",
    "# Use multiple adapters.\n",
    "model.add_adapter(peft_config, adapter_name='adapter1')\n",
    "model.add_adapter(peft_config, adapter_name='adapter2')\n",
    "\n",
    "model.set_adapter('adapter1')           # Set.\n",
    "model.disable_adapters(['adapter1'])    # Disable.\n",
    "model.enable_adapters(['adapter1'])     # Enable again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003e8135-c490-43fa-84d5-d9891aff0bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 1,231,940,608 || trainable%: 0.1915\n"
     ]
    }
   ],
   "source": [
    "# Num of trainable parameters.\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e15b6-e2a2-4373-9027-437814c23fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just use like typical model.\n",
    "# Training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"your-name/bigscience/mt0-large-lora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train.\n",
    "trainer.train()\n",
    "\n",
    "# Save the model.\n",
    "model.save_pretrained(\"output_dir\")\n",
    "\n",
    "# Push to hub.\n",
    "from huggingface_hub import login\n",
    "hf_token = \"your-huggingface-token\"\n",
    "login(token=hf_token)\n",
    "\n",
    "model.push_to_hub(model_id=\"your-name/bigscience/mt0-large-lora\")\n",
    "\n",
    "# Load.\n",
    "from peft import AutoPeftModel\n",
    "model = AutoPeftModel.from_pretrained(\"ybelkada/opt-350m-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc4d55e-c938-4e02-a7db-73f295c95369",
   "metadata": {},
   "source": [
    "> Note) Only save and load `Extra PEFT Parameters`, super efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd680948-05f5-4a6d-9234-c3353e3e60d8",
   "metadata": {},
   "source": [
    "## 2.3. Merge.\n",
    "- TIES, TrIm, Elect, and Merge.\n",
    "- DARE, Drop And REscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28048b34-6449-4b5d-95f1-47b2cf1cf1b4",
   "metadata": {},
   "source": [
    "> Note) When youâ€™re attempting to merge fully trained models with TIES, you should be aware of any special tokens each model may have added to the embedding layer which are not a part of the original checkpointâ€™s vocabulary. This may cause an issue because each model may have added a special token to the same embedding position. If this is the case, you should use the resize_token_embeddings method to avoid merging the special tokens at the same embedding index.\n",
    "> \n",
    "> This shouldnâ€™t be an issue if youâ€™re only merging LoRA adapters trained from the same base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7810bb-0e36-4ef7-9b32-b6b94d1e62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"smangrul/tinyllama_lora_norobots\", adapter_name=\"norobots\")\n",
    "model.load_adapter(\"smangrul/tinyllama_lora_sql\", adapter_name=\"sql\")\n",
    "model.load_adapter(\"smangrul/tinyllama_lora_adcopy\", adapter_name=\"adcopy\")\n",
    "\n",
    "adapters     = [\"norobots\", \"adcopy\", \"sql\"]\n",
    "weights      = [2.0, 1.0, 1.0]\n",
    "adapter_name = \"merge\"\n",
    "density      = 0.2\n",
    "model.add_weighted_adapter(adapters, weights, adapter_name, \n",
    "                           combination_type=\"ties\",            # \"dare_ties\" for DARE.\n",
    "                           density=density)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1c5d9-d4e2-4e2e-8dfe-57037354c4da",
   "metadata": {},
   "source": [
    "# 3. Bitsandbytes.\n",
    "- Decrease the float precision, with minimum tradeoff of performance.\n",
    "- Use '4-bits' as a default for large LLM.\n",
    "- Consider 'INT8' if you want better performance, sacrificing some training time and memory usage.\n",
    "\n",
    "> Note) 'LoRA' is compatible with quantization, but other PEFTs like prefix-tuning or prompt-based one may not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c9f29-9b7e-4a4d-a039-6a8567542f3f",
   "metadata": {},
   "source": [
    "## 3.1. StableEmbedding() Wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f6420-b91a-4be7-bf68-19dd0d13b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the embedding layer of the pretrained model with StableEmbedding.\n",
    "from bitsandbytes import StableEmbedding\n",
    "model.get_input_embeddings = StableEmbedding(model.get_input_embeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba8dca-641a-4155-aa58-c019cee48eb6",
   "metadata": {},
   "source": [
    "## 3.2. 8-bits Optimizer.\n",
    "- Full List of Optimizers : https://huggingface.co/docs/bitsandbytes/reference/optim/optim_overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b9081-7363-427c-8392-7fb0d78b4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-bits optimizer, using TrainingArguments and Trainer.\n",
    "from bitsandbytes import optim as bnb\n",
    "opt_adamw8 = bnb.optim.AdamW8bit(model.parameters(),      # Use AdamW8bit optimizer.     \n",
    "                                lr=5e-5,\n",
    "                                min_8bit_size=16384)      # All tensors with less than `min_8bit_size` elements are maintained as 32-bits float.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c18341-8985-43b6-b249-7745a4a27cd1",
   "metadata": {},
   "source": [
    "## 3.3. K-bits Quantized Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0697c30-16a9-4064-bc9d-05980a29432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bits Quantization.\n",
    "from transformers import BitsAndBytesConfig\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Make a model with quantization_config.\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", \n",
    "    quantization_config=config\n",
    ")\n",
    "\n",
    "# GlobalOptimManager : You can also specify parameters to use 32-bits manually.\n",
    "mng = bnb.optim.GlobalOptimManager.get_instance()\n",
    "mng.register_parameters(model.parameters())\n",
    "mng.override_config(model.fc1.weight, \"optim_bits\", 32)   # fc1.weight now uses 32-bit floats.\n",
    "\n",
    "# Linear8bitLt : Or using INT8 Quantization.\n",
    "def replace_linear_with_8bit(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):  # Replace all Linear layers to INT8 layer.\n",
    "            setattr(model, name, bnb.nn.Linear8bitLt(module.in_features, module.out_features))\n",
    "    return model\n",
    "    \n",
    "model = replace_linear_with_8bit(model)          # Replace linear layers in the model.\n",
    "\n",
    "# PEFT : LoRA.\n",
    "# Prepare PEFT model for quantized training.\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config.\n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# Training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    optim=\"adamw_bnb_8bit\",              # AdamW 8-bits optimizer.\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    optimizers=[opt_adamw8]             # AdamW 8-bits optimizer.\n",
    ")\n",
    "\n",
    "# The model is now prepared for **4-bit quantization + 8-bit Linear layers + LoRA fine-tuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fc5c4-da19-4a35-9e6a-3a485eecc2ab",
   "metadata": {},
   "source": [
    "> #### Note) Don't need to specify `BitsAndBytesConfig()`, just use `optim=\"adamw_bnb_8bit\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109899c-e5c5-4c05-96ad-5485821ccd93",
   "metadata": {},
   "source": [
    "# 4. Practical Guidelines for Large LLMs.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "- Hyperparameter tuning is **less critical** for pretrained models compared to customized models.\n",
    "- If needed, use **Optuna** for efficient and automated hyperparameter tuning.\n",
    "\n",
    "## Quantization.\n",
    "- Use **4-bit quantization** by default for efficient memory usage and faster computation.\n",
    "- Replace layers with **INT8 quantization** if you need better performance, trading off some computation and memory resources.\n",
    "\n",
    "## PEFT.\n",
    "- Use **LoRA** as the default PEFT method for most fine-tuning tasks.\n",
    "- In **Multitask Systems**:\n",
    "  - Train **multiple adapters separately** for each task.\n",
    "  - **Merge adapters** to reduce complexity and enable efficient multitask inference.\n",
    "- If you have **limited data** and are considering **zero-shot or few-shot learning**, use **Prompt-based** or **IA3** methods instead.\n",
    "- Be cautious with **quantization compatibility** when using Prompt-based or IA3 methods, as these may not work well with low-bit precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a8255-02dd-4bc6-a383-004af75c8691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
