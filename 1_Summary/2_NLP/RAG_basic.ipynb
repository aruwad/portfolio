{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd3d6ae-4ebb-4281-81b0-ccfa61a91c00",
   "metadata": {},
   "source": [
    "# 0. Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f23b3e-2d69-434e-8982-4f0f4a157103",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda.\n"
     ]
    }
   ],
   "source": [
    "# Imports.\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set random seed.\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Find device.\n",
    "device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device = {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c838a1d-c21b-461a-bde1-f9a8d84f2fc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        float: left;\n",
       "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70def0-75b6-4171-905c-740578ea7aaf",
   "metadata": {},
   "source": [
    "# 1. Introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa90698-aa56-4380-9d8f-20658e31a598",
   "metadata": {},
   "source": [
    "# 2. Document Preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ee2a7-0b58-486e-8fb9-68ecf9ce7d3d",
   "metadata": {},
   "source": [
    "## 2.1. Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb4dfa8-4c83-4ab6-b034-c5cf867556f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sample docs for DB vectors, as a simple str format.\n",
    "docs = [\"\"\"ChatGPT is a generative artificial intelligence chatbot[2][3] developed by OpenAI and launched in 2022. \n",
    "It is currently based on the GPT-4o large language model (LLM). ChatGPT can generate human-like conversational \n",
    "responses and enables users to refine and steer a conversation towards a desired length, format, style, level of \n",
    "detail, and language.[4] It is credited with accelerating the AI boom, which has led to ongoing rapid investment \n",
    "in and public attention to the field of artificial intelligence (AI).[5] Some observers have raised concerns \n",
    "about the potential of ChatGPT and similar programs to displace human intelligence, enable plagiarism, or fuel \n",
    "misinformation.\"\"\",\n",
    "\n",
    "\"\"\"By January 2023, ChatGPT had become what was then the fastest-growing consumer software application in history, \n",
    "gaining over 100 million users in two months[8][9] and contributing to the growth of OpenAI's current valuation of \n",
    "$86 billion.[10][11] ChatGPT's release spurred the development of competing products, including Gemini, Claude, \n",
    "Llama, Ernie, and Grok.[12] Microsoft launched Copilot, initially based on OpenAI's GPT-4. In May 2024, a partnership \n",
    "between Apple Inc. and OpenAI was announced, in which ChatGPT was integrated into the Apple Intelligence feature of \n",
    "Apple operating systems.[13] As of July 2024, ChatGPT's website is among the 10 most-visited websites globally.[14][15]\"\"\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d224779-de5f-4da2-b0df-08611683dd44",
   "metadata": {},
   "source": [
    "## 2.2. Split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a4a53-3b0b-43d1-9820-564dd9960ca0",
   "metadata": {},
   "source": [
    "### 2.2.1. `RecursiveCharacterTextSplitter`.\n",
    "- Excellent default.\n",
    "- Recursively splits text with delimters `separators` = [\"d_1\", \"d_2\", ...].\n",
    "- i.e. if len > chunk_size, split by \"d_1\", if still too long, split by \"d_2\", and so on.\n",
    "- e.g.\n",
    "  - \"Paragraph 1.\\nParagraph 2.\\nParagraph 3.\"  \n",
    "  - [\"Paragraph 1.\", \"Paragraph 2.\", \"Paragraph 3.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749a85ad-d86a-4fc3-aebb-ebb3f38188c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "page_content='ChatGPT is a generative artificial intelligence' metadata={'start_index': 0}\n",
      "\n",
      "Chunk 2:\n",
      "page_content='chatbot[2][3] developed by OpenAI and launched in' metadata={'start_index': 48}\n",
      "\n",
      "Chunk 3:\n",
      "page_content='in 2022' metadata={'start_index': 95}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Init splitter.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators       = [\". \", \" \"],   # First split by sentences, then by \" \".\n",
    "    chunk_size       = 50,            # Maximum chunk size.\n",
    "    chunk_overlap    = 5,             # Overlaps btw chunks.\n",
    "#    length_function  = lambda text: len(tokenizer.encode(text)),  # Define how to measure chunk_size. Default: character count.\n",
    "    keep_separator   = False,         # DO NOT USE IT, it can produce unexpected result. \n",
    "    add_start_index  = True,          # If `True`, stores position at chunks[0].metadata[\"start_index\"]. Default: `False`.\n",
    "    strip_whitespace = True,          # If `True`, delete leading/trailing spaces. Default: `True`.\n",
    ")\n",
    "\n",
    "# Split into chunks.\n",
    "chunks = text_splitter.create_documents(docs)\n",
    "\n",
    "# Print the result.\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7c39f-fa67-4b1e-892e-3a9236d8b534",
   "metadata": {},
   "source": [
    "### 2.2.2. `CharacterTextSplitter`.\n",
    "- Splits by the specified character.\n",
    "- Example:\n",
    "  - Document: \"12345678\".\n",
    "  - separator = \"5\", chunk_size = 3, chunk_overlap = 1.\n",
    "  - Steps:\n",
    "      1. Split by separator `\"5\"`.  \n",
    "         - Original: \"12345678\".  \n",
    "         - After splitting: [\"1234\", \"678\"].\n",
    "      2. Chunk each part with `chunk_size=3`, `chunk_overlap=1`.  \n",
    "         - \"1234\" â†’ [\"123\", \"34\"].  \n",
    "         - \"678\" â†’ [\"678\"].\n",
    "      3. Final output.  \n",
    "         - [\"123\", \"34\", \"678\"].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914118d2-d788-44db-ad17-54f1fc4ece45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "page_content='ChatGPT is a generative artificial intelligence'\n",
      "\n",
      "Chunk 2:\n",
      "page_content='chatbot[2][3] developed by OpenAI and launched in'\n",
      "\n",
      "Chunk 3:\n",
      "page_content='in 2022. \n",
      "It is currently based on the GPT-4o'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator     = \" \",\n",
    "    chunk_size    = 50,\n",
    "    chunk_overlap = 10\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents(docs)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2dbba6-3d82-4813-84d5-f83d2d5d77e4",
   "metadata": {},
   "source": [
    "### 2.2.3. `TokenTextSpliter`.\n",
    "- Splits by the token, from the specified tokenizer, `encoding_name`.\n",
    "- Useful for models with token limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee7ebdea-4cd7-43d2-80d0-246d844c4c4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: ChatGPT is a\n",
      "Chunk 2:  a generative artificial intelligence\n",
      "Chunk 3:  intelligence chatbot[2\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Example tokenizer.\n",
    "\n",
    "# Splitter with tokenizer.\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size     = 5,      \n",
    "    chunk_overlap  = 1,      \n",
    "    encoding_name  = \"gpt2\"  # Defines which tokenizer to use\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = text_splitter.split_text(docs[0])    # Note) input should be a single str, not a list of str.\n",
    "\n",
    "# Print results\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca2a76-c432-4ad6-9a36-23dee18122b1",
   "metadata": {},
   "source": [
    "### 2.2.4. Specific Formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f74e5-e112-4214-87fa-8dd82a42ab9b",
   "metadata": {},
   "source": [
    "#### 2.2.4.1. For Markdown.\n",
    "- `MarkdownHeaderTextSplitter`: Splits Markdown text based on header levels, preserving document structure.  \n",
    "   - \"# Header 1  \n",
    "     Content under header 1  \n",
    "     \\## Header 2  \n",
    "     Content under header 2\"  \n",
    "   - [\"# Header 1\\nContent under header 1\", \"## Header 2\\nContent under header 2\"]\n",
    "\n",
    "#### 2.2.4.2. For HTML.\n",
    "- `HTMLHeaderTextSplitter`: Splits HTML content based on header tags, maintaining the hierarchical structure.  \n",
    "   - \"\\<h1\\>Title\\</h1\\>  \n",
    "     \\<p\\>Intro paragraph.\\</p\\>  \n",
    "     \\<h2\\>Subtitle\\</h2\\>  \n",
    "     \\<p\\>Details.\\</p\\>\"  \n",
    "   - [\"\\<h1\\>Title\\</h1\\>  \n",
    "     \\<p\\>Intro paragraph.\\</p\\>\",  \n",
    "     \"\\<h2\\>Subtitle\\</h2\\>  \n",
    "     \\<p\\>Details.\\</p\\>\"]\n",
    "\n",
    "#### 2.2.4.3. For .py.\n",
    "- `PythonCodeTextSplitter`: Splits Python code into chunks based on logical code structures like functions and classes.  \n",
    "   - \"def func1():  \n",
    "       pass  \n",
    "\n",
    "     def func2():  \n",
    "       pass\"  \n",
    "   - [\"def func1():  \n",
    "       pass\",  \n",
    "       \"def func2():  \n",
    "       pass\"]\n",
    "\n",
    "#### 2.2.4.4. For .JSON.\n",
    "- `RecursiveJsonSplitter`: Recursively splits JSON data into smaller chunks while preserving the hierarchical structure.  \n",
    "   - '{\"key1\": {\"subkey\": \"value\"},  \n",
    "     \"key2\": \"value2\"}'  \n",
    "   - ['{\"key1\": {\"subkey\": \"value\"}}',  \n",
    "     '{\"key2\": \"value2\"}']\n",
    "\n",
    "#### 2.2.4.5. `SpacyTextSplitter`: \n",
    "   - Smart sentence splitter based on SpaCy.  \n",
    "   - \"This is the first sentence.  \n",
    "     This is the second sentence.\"  \n",
    "   - [\"This is the first sentence.\",  \n",
    "     \"This is the second sentence.\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be270562-950b-4e8e-9910-284b9fc99795",
   "metadata": {},
   "source": [
    "# 3. Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6e696-3977-4127-b231-6384b71e43c8",
   "metadata": {},
   "source": [
    "## 3.1 Embedding Models for Retrieval and Semantic Search\n",
    "\n",
    "| Model Series | Example Models | Pros | Cons |\n",
    "|-------------|---------------|------|------|\n",
    "| **Sentence Transformers** | `all-MiniLM-L6-v2` <br> Small, fast, and good for general embeddings. <br><br> `all-MPNet-base-v2` <br> More accurate but slower. <br><br> `nomic-ai/nomic-embed-text-v1` <br> Strong for document search. <br><br> `bge-large-en-v1.5` <br> Good for retrieval-augmented generation (RAG). | - No need for manual pooling. <br> - Efficient and optimized for semantic search. <br> - Many multilingual options. | - Less flexible than raw transformer models. <br> - Large models may be resource-intensive. |\n",
    "| **E5 Series (Optimized for Retrieval)** | `intfloat/multilingual-e5-small` <br> Small, fast, and supports multiple languages. <br><br> `intfloat/multilingual-e5-base` <br> Good for multilingual tasks. <br><br> `intfloat/multilingual-e5-large` <br> Stronger, but heavier. <br><br> `intfloat/e5-large-v2` <br> High-performance English embeddings. | - Optimized for FAISS and retrieval. <br> - Good for retrieval-augmented generation (RAG). <br> - English and multilingual versions available. | - Requires specific query formatting (`query: ...`, `passage: ...`). <br> - Larger models may be slow on CPU. |\n",
    "| **BGE Series (Best for RAG)** | `BAAI/bge-small-en` <br> Fast, good for retrieval. <br><br> `BAAI/bge-large-en` <br> Stronger, but requires more resources. <br><br> `BAAI/bge-m3` <br> Multilingual version. | - Strong retrieval performance. <br> - High efficiency for large-scale search. <br> - Good FAISS integration. | - Large models require more computational power. <br> - Not widely tested outside of retrieval. |\n",
    "| **GTE Series (Lightweight Embeddings)** | `thenlper/gte-small` <br> Smallest model, fast inference. <br><br> `thenlper/gte-large` <br> Better performance. | - Compact and efficient. <br> - Balanced speed and accuracy. | - Not as powerful as `bge` or `e5` for retrieval. <br> - Limited multilingual support. |\n",
    "| **Nomic AI (Best for Large Text)** | `nomic-ai/nomic-embed-text-v1` <br> Strong performance for large documents. | - Optimized for long-text retrieval. <br> - Designed for large-scale document search. | - Heavier than `MiniLM` or `GTE`. <br> - Requires high memory. |\n",
    "| **Cohere Embeddings (Commercial API)** | `cohere/embed-english-light-v3.0` <br> Lightweight but good for retrieval. <br><br> `cohere/embed-english-v3.0` <br> More powerful, requires API. | - Optimized for similarity tasks. <br> - Good for commercial applications. | - API-based, not open-source. <br> - Usage costs may apply. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeea7a4-4036-4b42-8567-bb4227c822c7",
   "metadata": {},
   "source": [
    "## 3.2. How to Choose.\n",
    "- General Use: `all-MiniLM-L6-v2`\n",
    "- Retrieval (FAISS, RAG): `bge-large-en-v1.5`, `e5-large-v2`\n",
    "- Multilingual: `intfloat/multilingual-e5-base`, `bge-m3`\n",
    "- Light Model: `gte-small`\n",
    "- Large Documents: `nomic-embed-text-v1`\n",
    "- Note) Be careful when mixing different embedding models, as FAISS retrieval results depend on consistent embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840d4c3-0e29-47ab-892b-fb96c762f508",
   "metadata": {},
   "source": [
    "## 3.3. Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36806e17-ecb3-4e27-a6df-5bb9496b96d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463b6de706aa4e3c80f510cc0b80e55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yana\\anaconda3\\envs\\study_ai\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yana\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81a1c4b97b0449d8f71afcd51504a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/498k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f56fed5230d44fab885510e0243a645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade206662baf46d4a6c867f79e16e5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78abe8ac22a544318c48860df09cd92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200c5c448f9f4a0fa6c0e202c7fecc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b850b901f34e1a8efe0fc222a5bd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b307b3ac3faa4ab89e33501a2199a6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d49d77b8c5e4bba928abcf6bc3952eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a49de65af4416e8554c68313dcbbc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding for 'e5-small' completed.\n",
      "- Chunk           : ChatGPT is a generative artificial intelligence chatbot[2][3] developed by OpenAI and launched in 2022\n",
      "- Embedding       : [ 0.03022079  0.00715949 -0.02565044 -0.02243968  0.04284703 -0.00995779\n",
      "  0.06023172  0.04781923  0.04110067 -0.04076944]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split into sentence.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators       = [\". \", \"\\n\"],   # First split by sentences, then by paragraphs.\n",
    "    chunk_size       = 100,            # Maximum chunk size.\n",
    "    chunk_overlap    = 10,             # Overlaps btw chunks.\n",
    "    strip_whitespace = True,           # If `True`, delete leading/trailing spaces. Default: `True`.\n",
    ")\n",
    "\n",
    "splits = text_splitter.create_documents(docs)\n",
    "chunks = [split.page_content for split in splits]\n",
    "\n",
    "# Embedding models.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_models = {\n",
    "    \"e5-small\": \"intfloat/multilingual-e5-small\"\n",
    "}\n",
    "\n",
    "# Initialize FAISS dictionary\n",
    "embeddings = {}\n",
    "\n",
    "# Process each model **one by one** (ensures logging after each step)\n",
    "for model_name, model_path in embedding_models.items():\n",
    "    # Load embedding model\n",
    "    embedding_model = SentenceTransformer(model_path, device=device)\n",
    "\n",
    "    # Encode all documents (ensures GPU usage if available)\n",
    "    embedding = embedding_model.encode(chunks)\n",
    "    \n",
    "    embeddings[model_name] = embedding\n",
    "\n",
    "    # âœ… Print log **immediately after each model is processed**\n",
    "    print(f\"âœ… Embedding for '{model_name}' completed.\")\n",
    "    print(f\"- {'Chunk':<15} : {chunks[0]}\")\n",
    "    print(f\"- {'Embedding':<15} : {embedding[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d0968-67d8-4789-a687-34d9c0f66a49",
   "metadata": {},
   "source": [
    "# 4. Vector DB Construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c273e-0397-48de-ab37-c79a3ce5e063",
   "metadata": {},
   "source": [
    "## 4.1. FAISS Index Function.\n",
    "- A **FAISS Index Function** defines the methodology on how to construct and search the VDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0c34e-65f1-4389-8908-865393fa40ef",
   "metadata": {},
   "source": [
    "## 4.2. Types.\n",
    "- **Exact Search:** Calculate a distance with **all** samples in a vector DB.\n",
    "\n",
    "  - **`IndexFlatL2` (L2-norm, Euclidean distance)**  \n",
    "    - $ d(\\mathbf{a}, \\mathbf{b}) = \\|\\mathbf{a} - \\mathbf{b}\\|_2 = \\sqrt{\\sum_{i} (a_i - b_i)^2} $\n",
    "\n",
    "  - **`IndexFlatIP` (Inner product, or Dot product similarity)**  \n",
    "    - $ \\text{d}(\\mathbf{a}, \\mathbf{b}) = \\sum_{i} a_i \\cdot b_i $\n",
    "    - If the input vectors are normalized, it calculates the cosine similarity.\n",
    "\n",
    "- **Efficient ANN:**\n",
    "  - **`IndexIVFFlat`**\n",
    "    - Perform a k-means clustering, search only nearest clusters.\n",
    "    - `nlist` = n_cluster $\\approx \\sqrt{n_{samples}}$, `nprobe` = n_search_clusters $\\approx 0.1 \\times n_{list}$\n",
    "\n",
    "  - **`IndexHNSWFlat`**\n",
    "    - Graph-based ANN search, constructs a multi-layer small-world network.\n",
    "    - Searches fewer nodes than brute-force while maintaining high recall.\n",
    "\n",
    "  - **`IndexPQ`**\n",
    "    - Compresses vectors using Product Quantization (PQ) to reduce memory.\n",
    "    - Approximates distance calculations by storing only compressed representations.\n",
    "\n",
    "  - **`IndexIVFPQ`**\n",
    "    - Combines `IndexIVFFlat` (clustering) and `IndexPQ` (compression).\n",
    "    - First reduces search space with IVF, then speeds up with PQ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761381b-4ecd-49e9-b9e1-930a263d17a8",
   "metadata": {},
   "source": [
    "## 4.3. How to Choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e54875-e6e1-4d42-8445-3e606639695e",
   "metadata": {},
   "source": [
    "| Index Type        | Speed  | Accuracy | Memory Usage | Best For |\n",
    "|------------------|--------|----------|--------------|----------|\n",
    "| **`IndexFlatL2`**  | Slow  | âœ…âœ…âœ… High  | ðŸ”º High  | Exact nearest neighbor search, small datasets |\n",
    "| **`IndexFlatIP`**  | Slow  | âœ…âœ…âœ… High  | ðŸ”º High  | Cosine similarity (with normalized vectors), exact search |\n",
    "| **`IndexIVFFlat`** | âš¡ Fast | âœ…âœ… Medium | ðŸ”¹ Medium | Large-scale search with clustering |\n",
    "| **`IndexHNSWFlat`** | âš¡âš¡âš¡ Very Fast | âœ…âœ…âœ… High | ðŸ”º High | High-speed, high-recall ANN search |\n",
    "| **`IndexPQ`**  | âš¡âš¡ Fast | âœ… Low | âœ…âœ… Low | Memory-efficient ANN search |\n",
    "| **`IndexIVFPQ`** | âš¡âš¡ Fast | âœ… Medium | âœ…âœ… Very Low | Large-scale ANN search with memory constraints |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050d61f-3b10-49f5-951a-88efe7ca10a8",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "- Use **`IndexFlatL2` / `IndexFlatIP`** for **exact search** when dataset size is small.\n",
    "- Use **`IndexIVFFlat`** for **large datasets**, tuning `nlist` and `nprobe` for speed vs. accuracy.\n",
    "- Use **`IndexHNSWFlat`** if you need **fastest ANN search with high recall**.\n",
    "- Use **`IndexPQ`** or **`IndexIVFPQ`** when **memory is limited**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373c813-ddf0-49c4-8d6f-f221618e4ab5",
   "metadata": {},
   "source": [
    "## 4.4. Example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f03e7dfd-addc-4dcb-92ec-dec9a5dbfe7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query: I enjoy studying artificial intelligence.\n",
      "\n",
      "ðŸ“„ Retrieved Documents (L2 Distance):\n",
      "- I love machine learning and AI.\n",
      "- Natural language processing is fascinating.\n",
      "\n",
      "ðŸ“„ Retrieved Documents (Inner Product):\n",
      "- I love machine learning and AI.\n",
      "- Natural language processing is fascinating.\n",
      "\n",
      "ðŸ“„ Retrieved Documents (IVF):\n",
      "- I love machine learning and AI.\n",
      "- Natural language processing is fascinating.\n",
      "\n",
      "ðŸ“„ Retrieved Documents (HNSW):\n",
      "- I love machine learning and AI.\n",
      "- Natural language processing is fascinating.\n",
      "\n",
      "ðŸ“„ Retrieved Documents (PQ):\n",
      "- I love machine learning and AI.\n",
      "- Natural language processing is fascinating.\n",
      "\n",
      "ðŸ“„ Retrieved Documents (IVF + PQ):\n",
      "- I love machine learning and AI.\n",
      "- Natural language processing is fascinating.\n",
      "Number of vectors in VDB: 5.\n"
     ]
    }
   ],
   "source": [
    "# Imports.\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Load sentence embedding model.\n",
    "model        = SentenceTransformer(\"all-MiniLM-L6-v2\")       # Small, fast transformer for sentence embeddings.\n",
    "\n",
    "# Sample database documents.\n",
    "docs         = [\n",
    "    \"The cat sits on the mat.\",\n",
    "    \"A dog is barking outside.\",\n",
    "    \"The weather is sunny today.\",\n",
    "    \"I love machine learning and AI.\",\n",
    "    \"Natural language processing is fascinating.\"\n",
    "]\n",
    "\n",
    "# Convert sentences to embeddings.\n",
    "d            = model.get_sentence_embedding_dimension()      # Get embedding dimension.\n",
    "xb           = np.array(model.encode(docs), dtype='float32') # Convert documents to vector representations.\n",
    "\n",
    "# Construct vector DBs.\n",
    "index_l2     = faiss.IndexFlatL2(d)     # L2 norm index.\n",
    "index_ip     = faiss.IndexFlatIP(d)     # Inner product index.\n",
    "\n",
    "nlist  = min(len(xb), 2)                # Number of clusters (recommended: âˆšN).\n",
    "nprobe = max(1, nlist // 2)             # Number of clusters to search.\n",
    "m_pq   = min(d, 2)\n",
    "n_bits = 2\n",
    "\n",
    "index_ivf    = faiss.IndexIVFFlat(faiss.IndexFlatL2(d), d, nlist)  # IVF with L2.\n",
    "index_hnsw   = faiss.IndexHNSWFlat(d, 32)                          # HNSW with 32 neighbors per node.\n",
    "index_pq     = faiss.IndexPQ(d, m_pq, n_bits)                      # PQ with 2 bits each, requiring 2^2=4 clusters.\n",
    "index_ivfpq  = faiss.IndexIVFPQ(faiss.IndexFlatL2(d), d, nlist, m_pq, n_bits)  # IVF + PQ.\n",
    "\n",
    "# Train IVF and PQ indexes.\n",
    "index_ivf.train(xb)   \n",
    "index_pq.train(xb)\n",
    "index_ivfpq.train(xb)  \n",
    "\n",
    "# Add sentence embeddings to FAISS.\n",
    "index_l2.add(xb)      \n",
    "index_ip.add(xb)      \n",
    "index_ivf.add(xb)     \n",
    "index_hnsw.add(xb)    \n",
    "index_pq.add(xb)      \n",
    "index_ivfpq.add(xb)   \n",
    "\n",
    "# Query example.\n",
    "query        = \"I enjoy studying artificial intelligence.\"   \n",
    "q            = np.array(model.encode([query]), dtype='float32')  # Convert query to embedding.\n",
    "\n",
    "# Find 'top_k' nearest neighbors.\n",
    "top_k        = 2  \n",
    "\n",
    "# Search and retrieval.\n",
    "D_l2, I_l2   = index_l2.search(q, top_k)   # D: distance from each retrieval, I: index of each retrieval.\n",
    "D_ip, I_ip   = index_ip.search(q, top_k)  \n",
    "\n",
    "index_ivf.nprobe = nprobe  # Set number of clusters to search.\n",
    "D_ivf, I_ivf     = index_ivf.search(q, top_k)  \n",
    "\n",
    "D_hnsw, I_hnsw   = index_hnsw.search(q, top_k)  \n",
    "D_pq, I_pq       = index_pq.search(q, top_k)  \n",
    "D_ivfpq, I_ivfpq = index_ivfpq.search(q, top_k)  \n",
    "\n",
    "# Display results.\n",
    "print(f\"ðŸ” Query: {query}\")\n",
    "\n",
    "print(\"\\nðŸ“„ Retrieved Documents (L2 Distance):\")\n",
    "for i in I_l2[0]:\n",
    "    print(f\"- {docs[i]}\")\n",
    "\n",
    "print(\"\\nðŸ“„ Retrieved Documents (Inner Product):\")\n",
    "for i in I_ip[0]:\n",
    "    print(f\"- {docs[i]}\")\n",
    "\n",
    "print(\"\\nðŸ“„ Retrieved Documents (IVF):\")\n",
    "for i in I_ivf[0]:\n",
    "    print(f\"- {docs[i]}\")\n",
    "\n",
    "print(\"\\nðŸ“„ Retrieved Documents (HNSW):\")\n",
    "for i in I_hnsw[0]:\n",
    "    print(f\"- {docs[i]}\")\n",
    "\n",
    "print(\"\\nðŸ“„ Retrieved Documents (PQ):\")\n",
    "for i in I_pq[0]:\n",
    "    print(f\"- {docs[i]}\")\n",
    "\n",
    "print(\"\\nðŸ“„ Retrieved Documents (IVF + PQ):\")\n",
    "for i in I_ivfpq[0]:\n",
    "    print(f\"- {docs[i]}\")\n",
    "\n",
    "# Store VDB.\n",
    "faiss.write_index(index_l2, \"./tmp/index_l2.faiss\")\n",
    "print(f\"Number of vectors in VDB: {index_l2.ntotal}.\")\n",
    "\n",
    "# Store docs.\n",
    "with open(\"./tmp/doc_rag_index_l2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d2f66-0e79-4362-be0e-9039d2d20280",
   "metadata": {},
   "source": [
    "# 5. Prompt Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05527e4-5734-44a7-8a75-32609160bf07",
   "metadata": {},
   "source": [
    "## 5.1. Basic Prompt Structure.\n",
    "- `Instruction`: Clearly define the task.\n",
    "- `Context`: Provide necessary background information.\n",
    "- `Input Data`: The specific input to be processed.\n",
    "- `Output Format`: Specify how the response should be structured.\n",
    "\n",
    "âœ” **Be explicit** â€“ Avoid vague instructions.  \n",
    "âœ” **Provide constraints** â€“ Define length, format, or style.  \n",
    "âœ” **Use natural language** â€“ Avoid overly technical language.  \n",
    "âœ” **Use step-by-step reasoning** â€“ Encourage logical outputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7b9de-1307-423e-b5fc-39067dcb3e54",
   "metadata": {},
   "source": [
    "## 5.2. Types of Prompting.\n",
    "- Different prompting strategies influence how the model generates responses. There are three main types:\n",
    "  - Zero-Shot Prompting.\n",
    "  - One-Shot Prompting.\n",
    "  - Few-Shot Prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e6627-c430-488e-83b9-9b7f910b56de",
   "metadata": {},
   "source": [
    "## 5.3. Fine-Tuning vs. Prompting.  \n",
    "\n",
    "| Approach          | Pros | Cons |\n",
    "|------------------|------|------|\n",
    "| **Prompt Optimization** | - Quickly learn a new task. <br>- Cost-effective. <br>- Works across multiple tasks. | - Limited control over the model.<br>- Performance may plateau. |\n",
    "| **Fine-Tuning** | - Can significantly improve accuracy.<br>- Customizable for specific tasks.<br>- Model adapts better to new data. | - Higher cost.<br>- Overfitting. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851813bb-f338-4f18-8234-d0fa4e484697",
   "metadata": {},
   "source": [
    "## 5.4. Advanced Prompt Techniques.\n",
    "\n",
    "### 5.4.1. CoT Prompting.\n",
    "\n",
    "- Chain-of-Thought (CoT) prompting breaks down complex problems into **step-by-step reasoning**, improving performance on logical and mathematical tasks.\n",
    "\n",
    "âœ… *Example (Without CoT)*:\n",
    "> Q: A farmer has 3 apples. He buys 5 more and eats 2. How many apples are left?  \n",
    "> A: 5.\n",
    "\n",
    "âœ… *Example (Few-Shot CoT)*:  \n",
    "> **Example 1:**  \n",
    "> Q: A farmer has 3 apples. He buys 5 more and eats 2. How many apples are left?  \n",
    "> A: Let's think step by step. The farmer starts with 3 apples. He buys 5 more, so he has 3 + 5 = 8 apples. Then he eats 2, so 8 - 2 = 6 apples left.  \n",
    "> **Final answer:** 6.  \n",
    ">  \n",
    "> **Example 2:**  \n",
    "> Q: A bookstore receives 120 books. It sells 45 books and then restocks with 30 more. How many books are in the store now?  \n",
    "> A: Let's think step by step. The bookstore starts with 120 books. It sells 45 books, so 120 - 45 = 75 books remain. Then it restocks 30 more, so 75 + 30 = 105 books.  \n",
    "> **Final answer:** 105.  \n",
    ">  \n",
    "> **Now, solve this:**  \n",
    "> Q: A bakery bakes 240 loaves of bread in the morning. It sells 90 loaves before noon and another 75 loaves in the afternoon. How many loaves are left at the end of the day?  \n",
    "> A: Let's think step by step.  \n",
    "\n",
    "ðŸ”¹ **Why use it?**\n",
    "- Improves reasoning for math, logic, and multi-step problems.\n",
    "- Commonly used in **question answering, problem-solving, and programming tasks**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4.2. Self-Consistency.\n",
    "\n",
    "- Self-consistency improves accuracy by generating multiple responses and selecting the most consistent answer.\n",
    "- Instead of relying on a single response, the model samples multiple outputs and aggregates the best one.\n",
    "\n",
    "âœ… *Example (Without Self-Consistency)*:\n",
    "> Q: What is 17 Ã— 23?  \n",
    "> A: 374.\n",
    "\n",
    "âœ… *Example (With Self-Consistency)*:\n",
    "> - First response: 391.  \n",
    "> - Second response: 374.  \n",
    "> - Third response: 391.  \n",
    "> - **Final answer: 391 (majority vote).**\n",
    "\n",
    "ðŸ”¹ **Why use it?**  \n",
    "- Reduces random errors in reasoning tasks.  \n",
    "- Increases stability for **math, logic, and factual questions**.  \n",
    "- Used in **LLM reasoning benchmarks and AI safety applications**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 5.4.3. ReAct (Reason + Act).\n",
    "\n",
    "- ReAct combines **reasoning** (thinking step-by-step) with **acting** (interacting with tools, APIs, or environments).\n",
    "- It enables LLMs to retrieve real-time information and make dynamic decisions.\n",
    "\n",
    "âœ… *Example (Without ReAct)*:\n",
    "> Q: What is the latest stock price of Tesla?  \n",
    "> A: I don't know.\n",
    "\n",
    "âœ… *Example (With ReAct using a search tool)*:\n",
    "> **Thought:** I need to check the latest Tesla stock price.  \n",
    "> **Action:** Searching online...  \n",
    "> **Observation:** Tesla stock price is $850.\n",
    "\n",
    "> **Final Answer:** The latest Tesla stock price is $850.\n",
    "\n",
    "ðŸ”¹ **Why use it?**  \n",
    "- Enables models to **retrieve and verify information dynamically**.  \n",
    "- Used in **chatbots, AI assistants, and research agents**.  \n",
    "- Key method for **LLMs integrated with APIs (LangChain, OpenAI Functions, Hugging Face Agents)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1bb104-74f9-4dc3-a599-d55245813d15",
   "metadata": {},
   "source": [
    "## 5.5. Examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a36ca-e847-4afa-bcb4-140d4a2ceff4",
   "metadata": {},
   "source": [
    "### 5.5.1. Retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb4e96a-eb9e-4159-a09e-dbf3b4d48e85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Load VDB.\n",
    "index_l2 = faiss.read_index('./tmp/index_l2.faiss')\n",
    "\n",
    "# Load documents.\n",
    "with open(\"./tmp/doc_rag_index_l2.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "# Query.\n",
    "model        = SentenceTransformer(\"all-MiniLM-L6-v2\")  \n",
    "\n",
    "query = \"I love cat.\"\n",
    "query_embedding = np.array(model.encode([query]), dtype='float32')\n",
    "\n",
    "# Retrieve.\n",
    "top_k     = 2\n",
    "distance, idx = index_l2.search(query_embedding, top_k)\n",
    "\n",
    "retrieved_docs = [f\"- {docs[i]}\" for i in idx[0] if i < len(docs)]\n",
    "retrieved_text = \"\\n\".join(retrieved_docs)  # Join retrieved docs first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d13c7-8701-4b68-88e2-cc26866aebcf",
   "metadata": {},
   "source": [
    "### 5.5.2. Basic Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec54e4c-b1b3-4f52-b49f-67f19c95483f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      "\n",
      "Context:\n",
      "- The cat sits on the mat.\n",
      "- I love machine learning and AI.\n",
      "\n",
      "Question: I love cat.\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_basic = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_basic = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template_basic\n",
    ")\n",
    "\n",
    "prompt_basic = prompt_template_basic.format(context=retrieved_text, question=query)\n",
    "\n",
    "print(\"Generated Prompt:\\n\")\n",
    "print(prompt_basic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57514176-5dae-4559-b53e-9cdbeb2ee6bb",
   "metadata": {},
   "source": [
    "### 5.5.3. CoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a9777dc-2d93-4bce-90aa-17e637e12932",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated CoT Prompt:\n",
      "\n",
      "Context:\n",
      "- The cat sits on the mat.\n",
      "- I love machine learning and AI.\n",
      "\n",
      "Example 1:\n",
      "Question: What is 2 + 2?\n",
      "Answer: Let's think step by step. We start with the number 2. Adding another 2 gives us 4.\n",
      "Final answer: 4.\n",
      "\n",
      "Example 2:\n",
      "Question: If a train travels at 60 km/h for 2 hours, how far does it go?\n",
      "Answer: Let's think step by step. The train's speed is 60 km/h. In 2 hours, it travels 60 Ã— 2 = 120 km.\n",
      "Final answer: 120 km.\n",
      "\n",
      "Now, answer the following question using the same reasoning:\n",
      "\n",
      "Question: I love cat.\n",
      "\n",
      "Answer:\n",
      "Let's think step by step to provide a well-reasoned answer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_cot = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Example 1:\n",
    "Question: What is 2 + 2?\n",
    "Answer: Let's think step by step. We start with the number 2. Adding another 2 gives us 4.\n",
    "Final answer: 4.\n",
    "\n",
    "Example 2:\n",
    "Question: If a train travels at 60 km/h for 2 hours, how far does it go?\n",
    "Answer: Let's think step by step. The train's speed is 60 km/h. In 2 hours, it travels 60 Ã— 2 = 120 km.\n",
    "Final answer: 120 km.\n",
    "\n",
    "Now, answer the following question using the same reasoning:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "Let's think step by step to provide a well-reasoned answer.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_cot = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template_cot\n",
    ")\n",
    "\n",
    "prompt_cot = prompt_template_cot.format(context=retrieved_text, question=query)\n",
    "\n",
    "print(\"Generated CoT Prompt:\\n\")\n",
    "print(prompt_cot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a63c9-c377-4fa0-b132-47933365dd70",
   "metadata": {},
   "source": [
    "### 5.5.4. Self-Consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5cf1ac5-3244-448e-89a2-49af6ea835fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer (Majority Vote): {answer_2}\n"
     ]
    }
   ],
   "source": [
    "# Hard voting example.\n",
    "\n",
    "# Use Counter.\n",
    "from collections import Counter\n",
    "\n",
    "# Extract final answers from responses.\n",
    "responses = {  # collect responses with multiple queries.\n",
    "    \"Question: {question_1} \\nAnswer: {answer_1}\",\n",
    "    \"Question: {question_2} \\nAnswer: {answer_2}\",\n",
    "    \"Question: {question_3} \\nAnswer: {answer_2}\",\n",
    "}\n",
    "answers = [resp.split(\"Answer:\")[-1].strip() for resp in responses if \"Answer:\" in resp]\n",
    "\n",
    "# Perform hard voting (majority vote).\n",
    "if answers:\n",
    "    most_common_answer, count = Counter(answers).most_common(1)[0]\n",
    "else:\n",
    "    print(\"No consistent answer found.\")\n",
    "    most_common_answer = answers[0]    # Just use 1st answer.\n",
    "\n",
    "print(\"Final Answer (Majority Vote):\", most_common_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9fcf1-07c9-47b9-9d74-35cc016172db",
   "metadata": {},
   "source": [
    "### 5.5.5. ReAct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a620ffe-517b-4922-9f50-a6309d4f150d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated ReAct Prompt:\n",
      "\n",
      "Context:\n",
      "- The cat sits on the mat.\n",
      "- I love machine learning and AI.\n",
      "\n",
      "Example 1:\n",
      "Question: What is the capital of France?\n",
      "Thought: The capital of France is well known. I will recall the information.\n",
      "Action: Answer directly\n",
      "Observation: Paris\n",
      "Final Answer: Paris\n",
      "\n",
      "Example 2:\n",
      "Question: What is the latest stock price of Tesla?\n",
      "Thought: I need real-time stock price information. I will search online.\n",
      "Action: Search[\"Tesla stock price\"]\n",
      "Observation: Tesla's stock price is $850.\n",
      "Final Answer: Tesla's stock price is $850.\n",
      "\n",
      "Now, follow the same reasoning process to answer the question.\n",
      "\n",
      "Question: I love cat.\n",
      "\n",
      "Thought:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Implement 'Action: Search[]' for practice.\n",
    "template_react = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Example 1:\n",
    "Question: What is the capital of France?\n",
    "Thought: The capital of France is well known. I will recall the information.\n",
    "Action: Answer directly\n",
    "Observation: Paris\n",
    "Final Answer: Paris\n",
    "\n",
    "Example 2:\n",
    "Question: What is the latest stock price of Tesla?\n",
    "Thought: I need real-time stock price information. I will search online.\n",
    "Action: Search[\"Tesla stock price\"]\n",
    "Observation: Tesla's stock price is $850.\n",
    "Final Answer: Tesla's stock price is $850.\n",
    "\n",
    "Now, follow the same reasoning process to answer the question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Thought:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_react = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template_react\n",
    ")\n",
    "\n",
    "prompt_react = prompt_template_react.format(context=retrieved_text, question=query)\n",
    "\n",
    "print(\"Generated ReAct Prompt:\\n\")\n",
    "print(prompt_react)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144d52f-b356-490a-8590-0bf4a568cebf",
   "metadata": {},
   "source": [
    "# 6. LLM for Answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0140d0b5-f429-4caf-9e83-63973e854aff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated CoT Prompt:\n",
      "\n",
      "Context:\n",
      "- The cat sits on the mat.\n",
      "- I love machine learning and AI.\n",
      "\n",
      "Example 1:\n",
      "Question: What is 2 + 2?\n",
      "Answer: Let's think step by step. We start with the number 2. Adding another 2 gives us 4.\n",
      "Final answer: 4.\n",
      "\n",
      "Example 2:\n",
      "Question: If a train travels at 60 km/h for 2 hours, how far does it go?\n",
      "Answer: Let's think step by step. The train's speed is 60 km/h. In 2 hours, it travels 60 Ã— 2 = 120 km.\n",
      "Final answer: 120 km.\n",
      "\n",
      "Now, answer the following question using the same reasoning:\n",
      "\n",
      "Question: I love cat.\n",
      "\n",
      "Answer:\n",
      "Let's think step by step to provide a well-reasoned answer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# Load VDB.\n",
    "index_l2 = faiss.read_index('./tmp/index_l2.faiss')\n",
    "\n",
    "# Load documents.\n",
    "with open(\"./tmp/doc_rag_index_l2.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "# Query.\n",
    "model        = SentenceTransformer(\"all-MiniLM-L6-v2\")  \n",
    "\n",
    "query = \"I love cat.\"\n",
    "query_embedding = np.array(model.encode([query]), dtype='float32')\n",
    "\n",
    "# Retrieve.\n",
    "top_k     = 2\n",
    "distance, idx = index_l2.search(query_embedding, top_k)\n",
    "\n",
    "retrieved_docs = [f\"- {docs[i]}\" for i in idx[0] if i < len(docs)]\n",
    "retrieved_text = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_cot = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Example 1:\n",
    "Question: What is 2 + 2?\n",
    "Answer: Let's think step by step. We start with the number 2. Adding another 2 gives us 4.\n",
    "Final answer: 4.\n",
    "\n",
    "Example 2:\n",
    "Question: If a train travels at 60 km/h for 2 hours, how far does it go?\n",
    "Answer: Let's think step by step. The train's speed is 60 km/h. In 2 hours, it travels 60 Ã— 2 = 120 km.\n",
    "Final answer: 120 km.\n",
    "\n",
    "Now, answer the following question using the same reasoning:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "Let's think step by step to provide a well-reasoned answer.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_cot = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template_cot\n",
    ")\n",
    "\n",
    "prompt_cot = prompt_template_cot.format(context=retrieved_text, question=query)\n",
    "\n",
    "print(\"Generated CoT Prompt:\\n\")\n",
    "print(prompt_cot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8798791c-d91d-44e6-b180-23832b44546f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "Context:\n",
      "- The cat sits on the mat.\n",
      "- I love machine learning and AI.\n",
      "\n",
      "Example 1:\n",
      "Question: What is 2 + 2?\n",
      "Answer: Let's think step by step. We start with the number 2. Adding another 2 gives us 4.\n",
      "Final answer: 4.\n",
      "\n",
      "Example 2:\n",
      "Question: If a train travels at 60 km/h for 2 hours, how far does it go?\n",
      "Answer: Let's think step by step. The train's speed is 60 km/h. In 2 hours, it travels 60 Ã— 2 = 120 km.\n",
      "Final answer: 120 km.\n",
      "\n",
      "Now, answer the following question using the same reasoning:\n",
      "\n",
      "Question: I love cat.\n",
      "\n",
      "Answer:\n",
      "Let's think step by step to provide a well-reasoned answer.\n",
      "Answer:\n",
      "But, this really doesn't happen! Suppose we get a list of 100 people running in a 100-min range,\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"distilgpt2\"  # or choose another model as needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, \n",
    "                          max_length=200, truncation=True)\n",
    "\n",
    "# Define the LLM using the updated HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_generator)\n",
    "\n",
    "# Simply generate the answer by invoking the LLM on prompt_cot:\n",
    "result = llm.invoke(prompt_cot)\n",
    "\n",
    "# Print the generated answer\n",
    "print(\"Generated Answer:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba832e-6aeb-438e-82e1-747230b0e1ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
