{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa6c9da-518b-42fb-8831-1961055df261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b2294-4ca9-4268-912a-8ee4890f2bf6",
   "metadata": {},
   "source": [
    "## Step 1: Identify Task Type.\n",
    "- **Understand Text**.\n",
    "  - Encoder-Only Transformers.\n",
    "- **Generative Tasks**.\n",
    "  - Decoder-Only Transformers.\n",
    "- **Transform Text to Text**.\n",
    "  - Encoder-Decoder Transformers.\n",
    "\n",
    "## Step 2: Computational Resources.\n",
    "- **Smaller models for less resources.**\n",
    "- **Number of Parameters**:\n",
    "  - Small: <100M.\n",
    "  - Medium: 100M ~ 500M.\n",
    "  - Large: >1B.\n",
    "- **Training and Inference Time**:\n",
    "  - Training Time : Important for fine-tuning.\n",
    "  - Inference Time : Important for deployment.\n",
    "  - `PyTorchBenchmark ` : ðŸ¤— library for measuring training and inference time.\n",
    "\n",
    "## Step 3: Fine-Tuning Requirements.\n",
    "- In general, larger model requires larger training data for fine-tuning.\n",
    "- **Small**:\n",
    "  - ~ 10,000.\n",
    "  - Examples:\n",
    "    - Few-shot learning tasks.\n",
    "    - Domain-specific tasks with limited labeled data (e.g., legal documents, rare languages).\n",
    "  - Use pre-trained models without extensive fine-tuning.\n",
    "  - Smaller models (e.g., DistilBERT, T5-Small) can reduce the risk of overfitting.\n",
    "  - Consider data augmentation.\n",
    "- **Medium**:\n",
    "  - ~ 1M.\n",
    "  - Examples:\n",
    "    - General-purpose datasets like SST-2 (67k sentences for sentiment analysis).\n",
    "    - Tasks like NER or extractive QA with moderate data availability.\n",
    "  - Find domain or task-specific base model and data for fine-tuning.\n",
    "  - Also consider evaluation on pre-trained objectives.\n",
    "- **Large**:\n",
    "  - 1M+.\n",
    "  - Examples:\n",
    "    - Massive datasets like C4 (T5 pre-training, ~750GB of text).\n",
    "    - Language modeling datasets (e.g., OpenAI's WebText for GPT).\n",
    "  - Consider distributed training across multiple GPUs or TPUs.\n",
    "\n",
    "## Step 4: Pretraining Data.\n",
    "- **General-Purpose Pretraining**: \n",
    "   - Models like **BERT**, **GPT-2**, or **DistilBERT** are trained on broad datasets and can be applied to most NLP tasks.\n",
    "- **Domain-Specific Pretraining**: \n",
    "   - If your task requires domain-specific knowledge (e.g., medical, legal, or financial tasks), look for models like **BioBERT** (biomedical), **LegalBERT** (legal), or fine-tune a general model on your domain-specific data.\n",
    " \n",
    "## Additional Checklist.\n",
    "\n",
    "### Consider Inference Speed.\n",
    "   - **Real-time applications** (chatbots, recommendation systems): \n",
    "     - Choose models optimized for fast inference like **DistilGPT-2**, **MobileBERT**, or **DistilBERT**.\n",
    "   - **Moderate speed required** (general tasks with balanced performance): \n",
    "     - **BERT**, **RoBERTa**, or **GPT-2**.\n",
    "   - **Slower models** (if not time-sensitive but need better accuracy): \n",
    "     - **T5**, **Pegasus**, or **GPT-3**.\n",
    "\n",
    "### Multilingual Model (If Needed).\n",
    "   - **Multilingual Tasks**: \n",
    "     - If your task involves multiple languages, choose models like **XLM-RoBerta**, **mBERT**, or **MarianMT**.\n",
    "   - **Single Language Task**: \n",
    "     - Use monolingual models like **BERT** or **GPT-2** if you're focused on one language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5322c5-2234-45a1-8851-b6e6475b5f09",
   "metadata": {},
   "source": [
    "# 2. Type of Transformers.\n",
    "\n",
    "## 2.1. Encoder-Only Models.\n",
    "- **Overview**: These models utilize only the encoder stack of the transformer architecture. They focus on understanding and contextualizing input text.\n",
    "- **Examples**: BERT, RoBERTa, DistilBERT.\n",
    "- **Strengths**:\n",
    "  - Ideal for tasks requiring deep understanding of text, such as text classification and named entity recognition (NER).\n",
    "  - Trained using Masked Language Modeling (MLM), which captures bidirectional context effectively.\n",
    "- **Limitations**:\n",
    "  - Not suitable for tasks requiring text generation, like story writing or dialogue generation.\n",
    "  - Produces static embeddings rather than generating novel text.\n",
    "\n",
    "\n",
    "| **Model**            | **HF Shortcut**  | **Pretraining Objectives** | **Tokenizer**      | **Parameters** | **Max Seq. Length** | **Training Speed** | **Inference Speed** | **Pretraining Data** | **Fine-Tuning Requirements** | **Task-Specific Performance** | **Special Features**                | **Useful Tasks**                   |\n",
    "|----------------------|------------------|----------------------------|--------------------|----------------|---------------------|--------------------|---------------------|---------------------|----------------------------|-----------------------------|-----------------------------------|-------------------------------------|\n",
    "| BERT (2018)         | `bert-base-uncased` | MLM, NSP                   | WordPiece          | 110M           | 512                 | Moderate          | Moderate            | General             | Moderate                   | Strong                     | Bidirectional context           | Text classification, NER, QA       |\n",
    "| RoBERTa (2019)      | `roberta-base`   | MLM                        | Byte-Pair          | 125M           | 512                 | Moderate          | Moderate            | Larger corpus       | Moderate                   | Strong                     | No NSP; improved over BERT      | Text classification, NER, QA       |\n",
    "| DistilBERT (2019)   | `distilbert-base-uncased` | MLM                        | WordPiece          | 66M            | 512                 | Fast              | Fast                | General             | Low                        | Moderate                   | Smaller, faster version of BERT | Text classification, NER, QA       |\n",
    "| ALBERT (2019)       | `albert-base-v2` | MLM, Sentence Ordering     | SentencePiece      | 12M            | 512                 | Fast              | Moderate            | General             | Low                        | Moderate                   | Parameter sharing, memory-efficient | Text classification, NER, QA       |\n",
    "| XLNet (2019)        | `xlnet-base-cased` | Permuted LM                | SentencePiece      | 117M           | 512                 | Slow              | Moderate            | General             | High                       | Strong                     | Handles permuted context        | Text classification, QA            |\n",
    "| Electra (2020)      | `electra-base-discriminator` | Replaced Token Detection   | WordPiece          | 110M           | 512                 | Moderate          | Moderate            | General             | Moderate                   | Strong                     | Efficient pretraining           | Text classification, QA            |\n",
    "| XLM-RoBERTa (2019)  | `xlm-roberta-base` | MLM                        | SentencePiece      | 270M           | 512                 | Slow              | Moderate            | Multilingual        | High                       | Strong (multilingual)      | Multilingual BERT               | Multilingual tasks, QA, NER        |\n",
    "| MobileBERT (2020)   | `mobilebert-uncased` | MLM, NSP                   | WordPiece          | 25M            | 512                 | Fast              | Fast                | General             | Low                        | Moderate                   | Optimized for mobile            | Text classification, NER, QA       |\n",
    "| CamemBERT (2019)    | `camembert-base` | MLM                        | SentencePiece      | 110M           | 512                 | Moderate          | Moderate            | French              | Moderate                   | Strong (French)            | French variant of RoBERTa       | French text processing, NER, QA    |\n",
    "| LayoutLM (2020)     | `layoutlm-base-uncased` | MLM                        | WordPiece          | 113M           | 512                 | Moderate          | Moderate            | Document Layout     | High                       | Specialized                | For document understanding      | Document analysis, form extraction |\n",
    "| Longformer (2020)   | `longformer-base-4096` | MLM                        | WordPiece          | 149M           | 4096                | Slow              | Moderate            | General             | High                       | Strong                     | Efficient attention for long texts | Long document QA, summarization   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb58dfa-021d-4bf3-ab76-ae1141c0fb45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## 2.3. Decoder-Only Models.\n",
    "- **Overview**: These models employ only the decoder stack and are optimized for autoregressive text generation.\n",
    "- **Examples**: GPT series (GPT-2, GPT-3, etc.), GPT-Neo.\n",
    "- **Strengths**:\n",
    "  - Excels in generative tasks, such as text completion, story generation, and conversational AI.\n",
    "  - Predicts the next token in a sequence, making it highly effective for language modeling.\n",
    "- **Limitations**:\n",
    "  - Less effective for tasks requiring bidirectional understanding of text, such as NER or extractive QA.\n",
    "  - Slower inference for longer sequences due to sequential token generation.\n",
    "\n",
    "| **Model**            | **HF Shortcut**  | **Pretraining Objectives** | **Tokenizer**      | **Parameters** | **Max Seq. Length** | **Training Speed** | **Inference Speed** | **Pretraining Data** | **Fine-Tuning Requirements** | **Task-Specific Performance** | **Special Features**                | **Useful Tasks**                   |\n",
    "|----------------------|------------------|----------------------------|--------------------|----------------|---------------------|--------------------|---------------------|---------------------|----------------------------|-----------------------------|-----------------------------------|-------------------------------------|\n",
    "| GPT-2 (2019)        | `gpt2`           | Autoregressive             | Byte-Pair          | 117M           | 1024                | Moderate          | Slow                | General             | Moderate                   | Moderate                   | Text generation focus           | Text generation, summarization     |\n",
    "| CTRL (2019)         | `ctrl`            | Autoregressive             | Byte-Pair          | 1.6B           | 512                 | Slow              | Slow                | General             | High                       | Strong                     | Controlled text generation      | Controlled text generation         |\n",
    "| DialoGPT (2019)     | `microsoft/DialoGPT-medium` | Autoregressive             | Byte-Pair          | 345M           | 1024                | Moderate          | Moderate            | Conversational      | Moderate                   | Moderate                   | Specialized for dialogues       | Dialogue systems, text generation  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6f6a1-9b95-4589-a600-6d8f149b89c7",
   "metadata": {},
   "source": [
    "## 2.4. Encoder-Decoder Models.\n",
    "- **Overview**: These models use both the encoder and decoder stacks, offering a balanced approach for understanding and generation.\n",
    "- **Examples**: T5, BART, mT5.\n",
    "- **Strengths**:\n",
    "  - Versatile, suitable for a wide range of tasks such as summarization, translation, and generative QA.\n",
    "  - Combines strong text understanding (encoder) with powerful text generation (decoder).\n",
    "- **Limitations**:\n",
    "  - Higher computational cost due to the dual-stack architecture.\n",
    "  - Requires careful fine-tuning to achieve optimal results for specific tasks.\n",
    "\n",
    "\n",
    "| **Model**            | **HF Shortcut**  | **Pretraining Objectives** | **Tokenizer**      | **Parameters** | **Max Seq. Length** | **Training Speed** | **Inference Speed** | **Pretraining Data** | **Fine-Tuning Requirements** | **Task-Specific Performance** | **Special Features**                | **Useful Tasks**                   |\n",
    "|----------------------|------------------|----------------------------|--------------------|----------------|---------------------|--------------------|---------------------|---------------------|----------------------------|-----------------------------|-----------------------------------|-------------------------------------|\n",
    "| T5 (2019)           | `t5-base`        | Text-to-Text               | SentencePiece      | 220M           | 512                 | Moderate          | Moderate            | General             | Moderate                   | Strong                     | Unified text-to-text tasks      | Text-to-text tasks, summarization  |\n",
    "| BART (2020)         | `facebook/bart-base` | Denoising Objectives       | Byte-Pair          | 140M           | 1024                | Moderate          | Moderate            | General             | Moderate                   | Strong                     | Flexible for text generation    | Summarization, QA                  |\n",
    "| MarianMT (2020)     | `Helsinki-NLP/opus-mt-en-ro` | Translation Task           | SentencePiece      | 61M            | 512                 | Moderate          | Moderate            | Multilingual        | Moderate                   | Moderate                   | Specialized for translation     | Translation                        |\n",
    "| Pegasus (2020)      | `google/pegasus-large` | Gap Sentence Generation    | SentencePiece      | 568M           | 512                 | Slow              | Moderate            | Summarization       | High                       | Strong                     | Specialized for summarization   | Summarization                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005f5db-2033-4d0c-8e86-89a1bc9177ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# 3. Criteria.\n",
    "\n",
    "## 3.1. **Type.**  \n",
    "  - **Encoder-Only.**\n",
    "    - Processes input sequences to extract meaningful representations. \n",
    "    - Understanding tasks : like text classification, NER, and text similarity.  \n",
    "  - **Decoder-Only.**\n",
    "    - Predicts tokens autoregressively based on the previous context.  \n",
    "    - Generative tasks : text generation, summarization, and dialogue systems.\n",
    "  - **Encoder-Decoder.**\n",
    "    - Encodes input sequences and generates outputs.   \n",
    "    - Versatile for both, but specialized for input-output mapping : such as translation, summarization, and generative QA.\n",
    "    - As both parts are required, it can be slower and more resource-intensive, especially for simpler tasks like text classification or NER.\n",
    "\n",
    "## 3.2. **Pretraining Objectives.**  \n",
    "  - **Masked Language Modeling (MLM).**  \n",
    "    - Helps understand bidirectional context by predicting randomly masked tokens.  \n",
    "    - Commonly used in models like BERT and RoBERTa.  \n",
    "  - **Next Sentence Prediction (NSP).**  \n",
    "    - Trains the model to understand relationships between sentences.  \n",
    "    - Used in BERT but removed in RoBERTa for optimization.  \n",
    "  - **Autoregressive Language Modeling.**  \n",
    "    - Predicts the next token based on previous tokens, ideal for generation tasks.  \n",
    "    - Used in decoder-only models like GPT.  \n",
    "  - **Text-to-Text.**  \n",
    "    - Converts all tasks into a unified text-to-text format, simplifying multi-task learning.  \n",
    "    - Central to models like T5.  \n",
    "  - **Sentence Ordering.**  \n",
    "    - Aims to predict the correct order of shuffled sentences.  \n",
    "    - Used in ALBERT for improved coherence understanding.  \n",
    "  - **Replaced Token Detection.**  \n",
    "    - Identifies tokens replaced by a generator network instead of standard masking.  \n",
    "    - Makes training more efficient in Electra.  \n",
    "  - **Gap Sentence Generation (GSG).**  \n",
    "    - Requires the model to generate text to fill gaps in input.  \n",
    "    - Emphasizes abstractive tasks and is used in Pegasus.  \n",
    "  - **Permutation Language Modeling (PLM).**  \n",
    "    - Predicts tokens from permuted input sequences, enabling bidirectional dependency.  \n",
    "    - Used in XLNet for improved autoregressive and bidirectional learning.  \n",
    "  - **Denoising Objectives.**  \n",
    "    - Reconstructs corrupted inputs by applying noise transformations like deletion or masking.  \n",
    "    - Central to BART for robust generative performance.  \n",
    "\n",
    "## 3.3. **Tokenizer.**  \n",
    "  - **WordPiece.**  \n",
    "    - Breaks text into subwords, balancing vocabulary size and granularity.  \n",
    "    - Useful Tasks : Text classification, NER, QA.  \n",
    "    - Typical Models : BERT, DistilBERT.  \n",
    "  - **Byte-Pair Encoding (BPE).**  \n",
    "    - Merges frequent character pairs into subwords, commonly used in generative models.  \n",
    "    - Useful Tasks : Text generation, summarization.  \n",
    "    - Typical Models : GPT, RoBERTa.  \n",
    "  - **SentencePiece.**  \n",
    "    - Uses unsupervised learning to tokenize text, supporting multilingual applications.  \n",
    "    - Useful Tasks : Translation, summarization.  \n",
    "    - Typical Models : T5, XLM-RoBERTa.  \n",
    "\n",
    "## 3.4. **Parameters.**\n",
    "  - **Small.**\n",
    "    - Less than 50M parameters; faster and more resource-efficient.\n",
    "    - Useful Tasks: Simple classification, small-scale tasks.\n",
    "    - Typical Models: DistilBERT, MobileBERT.\n",
    "  - **Medium.**\n",
    "    - 50â€“200M parameters; balances performance and efficiency.\n",
    "    - Useful Tasks: General-purpose tasks, moderate complexity.\n",
    "    - Typical Models: BERT, RoBERTa, MarianMT.\n",
    "  - **Large.**\n",
    "    - More than 200M parameters; captures complex patterns but requires more resources.\n",
    "    - Useful Tasks: Complex tasks, high accuracy demands.\n",
    "    - Typical Models: GPT-2, T5, Pegasus.\n",
    "\n",
    "## 3.5. **Max Sequence Length.**\n",
    "  - **Short.**\n",
    "    - Less than 512 tokens; sufficient for most tasks.\n",
    "    - Useful Tasks: Text classification, sentiment analysis.\n",
    "  - **Medium.**\n",
    "    - Up to 512 tokens; common for models like BERT.\n",
    "    - Useful Tasks: Named Entity Recognition (NER), QA.\n",
    "  - **Long.**\n",
    "    - Over 512 tokens; necessary for long documents or contexts (e.g., Longformer).\n",
    "    - Useful Tasks: Long document summarization, document analysis.\n",
    "\n",
    "## 3.6. **Training Speed.**\n",
    "  - **Fast.**\n",
    "    - Models like DistilBERT or MobileBERT, optimized for quick fine-tuning.\n",
    "    - Useful Tasks: Low-resource tasks, rapid deployment.\n",
    "  - **Moderate.**\n",
    "    - Standard models like BERT or RoBERTa.\n",
    "    - Useful Tasks: General-purpose tasks with a balance of efficiency.\n",
    "  - **Slow.**\n",
    "    - Larger models like RoBERTa-large or Pegasus, requiring more time to fine-tune.\n",
    "    - Useful Tasks: Complex tasks with significant resources.\n",
    "\n",
    "## 3.7. **Inference Speed.**\n",
    "  - **Real-Time Optimized.**\n",
    "    - Models like MobileBERT, suitable for low-latency applications.\n",
    "    - Useful Tasks: Real-time classification, dialogue systems.\n",
    "  - **General-Purpose.**\n",
    "    - Models like BERT or RoBERTa, offering a balance of speed and accuracy.\n",
    "    - Useful Tasks: General-purpose inference tasks.\n",
    "  - **Slow.**\n",
    "    - Generative models like GPT-2 or Pegasus, which may not suit real-time tasks.\n",
    "    - Useful Tasks: Creative text generation, summarization.\n",
    "\n",
    "## 3.8. **Pretraining Data.**\n",
    "  - **General-Purpose.**\n",
    "    - Trained on datasets like Wikipedia and BooksCorpus.\n",
    "    - Useful Tasks: Text classification, QA.\n",
    "  - **Multilingual.**\n",
    "    - Trained on diverse languages, enabling cross-lingual tasks (e.g., XLM-RoBERTa).\n",
    "    - Useful Tasks: Multilingual text processing, translation.\n",
    "  - **Domain-Specific.**\n",
    "    - Tailored for specific fields like biomedical or legal text.\n",
    "    - Useful Tasks: Specialized tasks in specific domains (e.g., medical text analysis).\n",
    "\n",
    "## 3.9. **Fine-Tuning Requirements.**\n",
    "  - **Low.**\n",
    "    - Small models for simple tasks (e.g., DistilBERT).\n",
    "    - Useful Tasks: Simple text classification, sentiment analysis.\n",
    "  - **Moderate.**\n",
    "    - Mid-size models for general-purpose tasks (e.g., BERT).\n",
    "    - Useful Tasks: General-purpose tasks, moderate complexity.\n",
    "  - **High.**\n",
    "    - Large models for complex tasks or when fine-tuning requires significant data.\n",
    "    - Useful Tasks: Complex language generation, translation, summarization.\n",
    "\n",
    "## 3.10. **Task-Specific Performance.**\n",
    "  - **Text Classification.**\n",
    "    - Tasks like IMDb sentiment analysis, where BERT excels.\n",
    "  - **Text Generation.**\n",
    "    - Tasks like dialogue systems or story generation, suited for GPT-like models.\n",
    "  - **Summarization.**\n",
    "    - Best handled by models like BART or Pegasus.\n",
    "  - **Translation.**\n",
    "    - Specialized models like MarianMT or encoder-decoder architectures like T5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72fa4d-759d-4afb-acc5-40ebaae895cd",
   "metadata": {},
   "source": [
    "# 4. Examples.\n",
    "\n",
    "## Example 1: Sentiment Analysis.\n",
    "- **Task**: Classify customer reviews as positive, negative, or neutral.\n",
    "- **Dataset Size**: 50,000 labeled examples.\n",
    "- **Resources**: Single GPU with 8GB VRAM.\n",
    "- **Deployment Needs**: Moderate inference speed.\n",
    "- **Model Choice**: **DistilBERT**, **ALBERT**, or **MobileBERT**.\n",
    "  - Encoder-only models are ideal for classification tasks.\n",
    "  - Lightweight models are chosen due to limited resources and small dataset size.\n",
    "  - **Recommendation**: \n",
    "    - Use **DistilBERT** for simplicity.\n",
    "    - Use **ALBERT** for better memory efficiency.\n",
    "    - Use **MobileBERT** for faster inference.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 2: Text Summarization.\n",
    "- **Task**: Summarize news articles into concise summaries.\n",
    "- **Dataset Size**: 1 million articles with summaries.\n",
    "- **Resources**: GPU cluster for training but limited resources for deployment.\n",
    "- **Deployment Needs**: High-quality summaries; inference speed is secondary.\n",
    "- **Model Choice**: **T5** or **Pegasus**.\n",
    "  - Encoder-decoder models are suitable for summarization.\n",
    "  - **Recommendation**: \n",
    "    - Use **Pegasus** for specialized summarization tasks.\n",
    "    - Use **T5** for broader flexibility across tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 3: Multilingual Machine Translation.\n",
    "- **Task**: Translate texts between English, French, and German.\n",
    "- **Dataset Size**: 10 million parallel sentences.\n",
    "- **Resources**: Access to TPUs for training.\n",
    "- **Deployment Needs**: Fast inference for real-time translation.\n",
    "- **Model Choice**: **MarianMT**.\n",
    "  - Designed explicitly for translation with strong multilingual capabilities.\n",
    "  - Optimized for fast inference, meeting real-time deployment needs.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 4: RTX 3070.\n",
    "\n",
    "### Fine-Tuning (Training Mode):\n",
    "- **With 32-bit precision**:\n",
    "  - Supports ~300M to 400M parameters (e.g., BERT-Base, RoBERTa-Base).\n",
    "- **With 16-bit mixed precision**:\n",
    "  - Supports ~600M to 700M parameters (e.g., T5-Base, GPT-2 Medium).\n",
    "- **Batch size and sequence length impact**:\n",
    "  - Larger batch sizes or longer sequences will reduce the maximum parameter count.\n",
    "  - Smaller batch sizes or shorter sequences allow for slightly larger models.\n",
    "- **BERT-Base** (~110M parameters):\n",
    "  - Easily fits with moderate batch sizes (16â€“32) and sequence lengths (up to 512 tokens).\n",
    "- **GPT-2 Medium** (~345M parameters):\n",
    "  - Fits with mixed precision and smaller batch sizes (~8â€“16).\n",
    "\n",
    "### Inference Mode:\n",
    "- Supports ~1B parameters (e.g., GPT-2 XL) for single-sequence inference.\n",
    "- Inference requires less memory since there are no gradients or optimizer states.\n",
    "- **T5-Large** (~770M parameters):\n",
    "  - Possible for inference but may require reducing batch size or sequence length.\n",
    "- **GPT-2 XL** (~1.5B parameters):\n",
    "  - Might work for inference with single sequences using optimizations like offloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129df93-0581-438d-be64-85939f1c7f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
