{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903309f2-c0b3-444a-bf3e-322896de5036",
   "metadata": {},
   "source": [
    "# 0. Setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac82f79-631d-47f7-b1f4-80eb73f7a508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        float: left;\n",
       "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5306df1-be9f-42d2-af22-f5573f7d1552",
   "metadata": {},
   "source": [
    "# 1. `TrainingArguments`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e4191-2b7a-4321-b1af-c0acd8d7daec",
   "metadata": {},
   "source": [
    "## 1. General Setup.\n",
    "\n",
    "|   | Name                     | Default   | Values          | Description                             |\n",
    "|:--|:-------------------------|:----------|:----------------|:----------------------------------------|\n",
    "| 1 | `output_dir`             | *None*    | *String (path)* | Directory to save results, checkpoints. |\n",
    "| 2 | `overwrite_output_dir`   | *False*   | *Boolean*       | Overwrite existing output directory.    |\n",
    "| 3 | `prediction_loss_only`   | *False*   | *Boolean*       | Only return loss during evaluation.     |\n",
    "| 4 | `run_name`               | *None*    | *String*        | Identifier for the training run.        |\n",
    "| 5 | `disable_tqdm`           | *None*    | *Boolean*       | Disable tqdm progress bars.             |\n",
    "| 6 | `label_names`            | *None*    | *List of str*   | List of label names for the model.      |\n",
    "| 7 | `load_best_model_at_end` | *False*   | *Boolean*       | Load best model at the end of training. |\n",
    "| 8 | `metric_for_best_model`  | *None*    | *String*        | Metric to use for selecting best model. |\n",
    "| 9 | `greater_is_better`      | *None*    | *Boolean*       | Whether better models have greater metric values. |\n",
    "| 10| `ignore_data_skip`       | *False*   | *Boolean*       | Ignore data skipping when resuming training. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f4855-b2d6-498a-97c4-ce16169f750b",
   "metadata": {},
   "source": [
    "## 2. Training Control.\n",
    "\n",
    "|   | Name                            | Default   | Values        | Description                                  |\n",
    "|---|---------------------------------|-----------|---------------|----------------------------------------------|\n",
    "| 1 | `num_train_epochs`              | *3*       | *Float*       | Total number of training epochs.             |\n",
    "| 2 | `per_device_train_batch_size`   | *8*       | *Integer*     | Batch size per device during training.       |\n",
    "| 3 | `per_device_eval_batch_size`    | *8*       | *Integer*     | Batch size per device during evaluation.     |\n",
    "| 4 | `gradient_accumulation_steps`   | *1*       | *Integer*     | Number of steps to accumulate gradients.     |\n",
    "| 5 | `warmup_steps`                  | *0*       | *Integer*     | Number of steps for learning rate warmup.    |\n",
    "| 6 | `weight_decay`                  | *0.0*     | *Float*       | Weight decay for AdamW optimizer.            |\n",
    "| 7 | `max_grad_norm`                 | *1.0*     | *Float*       | Maximum norm for gradient clipping.          |\n",
    "| 8 | `logging_steps`                 | *500*     | *Integer*     | Frequency of logging during training.        |\n",
    "| 9 | `max_steps`                     | *-1*      | *Integer*     | If > 0, total number of training steps to run. |\n",
    "| 10| `save_steps`                    | *500*     | *Integer*     | Number of steps between saving checkpoints.  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48c89e-9942-4bba-b877-afad51c55676",
   "metadata": {},
   "source": [
    "## 3. Evaluation and Logging.\n",
    "\n",
    "|   | Name                      | Default   | Values        | Description                                |\n",
    "|---|:--------------------------|:----------|:--------------|--------------------------------------------|\n",
    "| 1 | `evaluation_strategy`     | *\"no\"*    | *{\"no\", \"steps\", \"epoch\"}* | How often to run evaluation.             |\n",
    "| 2 | `eval_steps`              | *None*    | *Integer*     | Number of steps between evaluations.       |\n",
    "| 3 | `logging_dir`             | *\"runs/\"* | *String (path)* | Directory for saving TensorBoard logs.     |\n",
    "| 4 | `logging_steps`           | *500*     | *Integer*     | Frequency of logging metrics.              |\n",
    "| 5 | `save_strategy`           | *\"steps\"* | *{\"no\", \"steps\", \"epoch\"}* | When to save model checkpoints.            |\n",
    "| 6 | `save_steps`              | *500*     | *Integer*     | Number of steps between saving checkpoints.|\n",
    "| 7 | `save_total_limit`        | *None*    | *Integer*     | Maximum number of saved checkpoints.       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea3045-2963-4665-b5f8-9bb03b825fa2",
   "metadata": {},
   "source": [
    "## 4. Optimization.\n",
    "\n",
    "|   | Name                      | Default   | Values        | Description                                |\n",
    "|---|:--------------------------|:----------|:--------------|--------------------------------------------|\n",
    "| 1 | `learning_rate`           | *5e-5*    | *Float*       | Initial learning rate for the optimizer.   |\n",
    "| 2 | `lr_scheduler_type`       | *\"linear\"*| *{\"linear\", \"cosine\", \"polynomial\", \"constant\"}* | Type of learning rate scheduler. |\n",
    "| 3 | `weight_decay`            | *0.0*     | *Float*       | Weight decay coefficient.                  |\n",
    "| 4 | `adam_beta1`              | *0.9*     | *Float*       | Beta1 parameter for AdamW optimizer.       |\n",
    "| 5 | `adam_beta2`              | *0.999*   | *Float*       | Beta2 parameter for AdamW optimizer.       |\n",
    "| 6 | `adam_epsilon`            | *1e-8*    | *Float*       | Epsilon value for AdamW optimizer.         |\n",
    "| 7 | `warmup_steps`            | *0*       | *Integer*     | Number of warmup steps for the scheduler.  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070748f-552d-4602-a6a8-e3666bd53bc4",
   "metadata": {},
   "source": [
    "## 5. Hardware Utilization.\n",
    "\n",
    "|   | Name                      | Default   | Values        | Description                                |\n",
    "|---|:--------------------------|:----------|:--------------|--------------------------------------------|\n",
    "| 1 | `no_cuda`                 | *False*   | *Boolean*     | Disable GPU usage.                         |\n",
    "| 2 | `fp16`                    | *False*   | *Boolean*     | Use 16-bit precision for training.         |\n",
    "| 3 | `fp16_opt_level`          | *\"O1\"*    | *{\"O0\", \"O1\", \"O2\", \"O3\"}* | Optimization level for fp16 training.    |\n",
    "| 4 | `local_rank`              | *-1*      | *Integer*     | Rank of the process during distributed training. |\n",
    "| 5 | `tpu_num_cores`           | *None*    | *Integer*     | Number of TPU cores to use.                |\n",
    "| 6 | `dataloader_num_workers`  | *0*       | *Integer*     | Number of subprocesses for data loading.   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59384190-6007-45bb-bc10-47d03c2f8164",
   "metadata": {},
   "source": [
    "# 2. `Trainer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07748d3c-9ad0-4d59-bfc8-d147f39e3464",
   "metadata": {},
   "source": [
    "|   | Name                            | Default   | Values        | Description                                  |\n",
    "|---|:---------------------------------|:----------|:--------------|----------------------------------------------|\n",
    "| 1 | `model`                          | *None*    | *PreTrainedModel or torch.nn.Module* | The model to train, evaluate or use for predictions. |\n",
    "| 2 | `args`                           | *None*    | *TrainingArguments* | Arguments to tweak for training. Defaults to a basic instance of `TrainingArguments`. |\n",
    "| 3 | `data_collator`                  | *None*    | *DataCollator* | Function to form a batch from dataset elements. |\n",
    "| 4 | `train_dataset`                  | *None*    | *torch.utils.data.Dataset or datasets.Dataset* | Dataset for training. |\n",
    "| 5 | `eval_dataset`                   | *None*    | *torch.utils.data.Dataset or datasets.Dataset* | Dataset for evaluation. |\n",
    "| 6 | `model_init`                     | *None*    | *Callable[[], PreTrainedModel]* | Function that initializes a new model for each training run. |\n",
    "| 7 | `compute_loss_func`              | *None*    | *Callable*    | Function that computes loss from model outputs and labels. |\n",
    "| 8 | `compute_metrics`                | *None*    | *Callable[[EvalPrediction], Dict]* | Function to compute metrics during evaluation. |\n",
    "| 9 | `callbacks`                      | *None*    | *List[TrainerCallback]* | List of callbacks to customize the training loop. |\n",
    "| 10 | `optimizers`                     | *None*    | *Tuple[Optimizer, lr_scheduler]* | Optimizer and scheduler to use. Defaults to AdamW and linear schedule. |\n",
    "| 11 | `optimizer_cls_and_kwargs`       | *None*    | *Tuple[Type[Optimizer], Dict]* | Custom optimizer class and arguments. |\n",
    "| 12 | `preprocess_logits_for_metrics`  | *None*    | *Callable[[torch.Tensor, torch.Tensor], torch.Tensor]* | Function to preprocess logits before evaluation. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfcca37-8ccb-4f3c-8cca-f2666b15568d",
   "metadata": {},
   "source": [
    "## 2.1. Example - `data_collator` for dynamic padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ac3bf-67fb-42c6-9778-43eb4662514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    return tokenizer.pad(batch, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe7647-46a0-403b-9b82-87db097c2209",
   "metadata": {},
   "source": [
    "# 3. Callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75154ee-defc-451e-bb05-b40ab401a09d",
   "metadata": {},
   "source": [
    "| Name                               | Parameters                                              | Description                                                                                                      |\n",
    "|:-----------------------------------|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|\n",
    "| `DefaultFlowCallback`              | .                                                       | Logs training metrics (e.g., loss, learning rate) to the directory specified by `logging_dir` (default is `./logs`). It also handles evaluation and checkpoint saving. |\n",
    "| `PrinterCallback`                  | .                                                       | Just prints the logs on the console.                                                                |\n",
    "| `ProgressCallback`                 | `max_str_len`: int = 100                                | Displays the progress of training or evaluation. `max_str_len` is how long strings are truncated when logging. |\n",
    "| `EarlyStoppingCallback`            | `early_stopping_patience`: int = 1, `early_stopping_threshold`: Optional[float] = 0.0 | Stop training after `early_stopping_patience` evaluations without improvement, more than `early_stopping_threshold`. |\n",
    "| `TensorBoardCallback`              | `tb_writer`: SummaryWriter = None                       | A `TrainerCallback` that sends the logs to TensorBoard. If `tb_writer` is not provided, it will instantiate one.  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ea091-426c-4bb1-b8e5-b18780056538",
   "metadata": {},
   "source": [
    "> #### Note) How to Run TensorBoard.\n",
    "> `tensorboard --logdir ./logs`.  \n",
    "> TrainingArguments(`logging_dir='./logs'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1f519-8146-4ab0-a537-e36d7a672239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
